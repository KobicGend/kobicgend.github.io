<!DOCTYPE html><html lang="zh-TW" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>小明の雜貨鋪 | 小明の雜貨鋪</title><meta name="author" content="小明同學"><meta name="copyright" content="小明同學"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="不同教材的符号体系略有差别，本笔记以浙江大学编，高等教育出版社出版的《概率论与数理统计》为参考，有少许修改。 本笔记的例题均选自重要课后习题或本校期末考试真题。 本笔记的个人倾向性较强，    §4.3.3　协方差矩阵在很多情况下，我们实际上会需要关心一个随机向量的很多分量之间的协方差。因此，我们对协方差做一个推广，利用矩阵来简便地表示一个随机向量中任意两个分量之间的协方差。这就是协方差矩阵。我们">
<meta property="og:type" content="article">
<meta property="og:title" content="小明の雜貨鋪">
<meta property="og:url" content="http://kobicgend.top/posts/0.html">
<meta property="og:site_name" content="小明の雜貨鋪">
<meta property="og:description" content="不同教材的符号体系略有差别，本笔记以浙江大学编，高等教育出版社出版的《概率论与数理统计》为参考，有少许修改。 本笔记的例题均选自重要课后习题或本校期末考试真题。 本笔记的个人倾向性较强，    §4.3.3　协方差矩阵在很多情况下，我们实际上会需要关心一个随机向量的很多分量之间的协方差。因此，我们对协方差做一个推广，利用矩阵来简便地表示一个随机向量中任意两个分量之间的协方差。这就是协方差矩阵。我们">
<meta property="og:locale" content="zh_TW">
<meta property="og:image" content="http://kobicgend.top/img/avatar.png">
<meta property="article:published_time" content="2024-05-31T09:51:59.802Z">
<meta property="article:modified_time" content="2024-06-14T04:53:50.589Z">
<meta property="article:author" content="小明同學">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://kobicgend.top/img/avatar.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://kobicgend.top/posts/0.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":1,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":1200},
  copy: {
    success: '複製成功',
    error: '複製錯誤',
    noSupport: '瀏覽器不支援'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '剛剛',
    min: '分鐘前',
    hour: '小時前',
    day: '天前',
    month: '個月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '載入更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '小明の雜貨鋪',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-06-14 12:53:50'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
          const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
          const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
          const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

          if (t === undefined) {
            if (isLightMode) activateLightMode()
            else if (isDarkMode) activateDarkMode()
            else if (isNotSpecified || hasNoSupport) {
              const now = new Date()
              const hour = now.getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            window.matchMedia('(prefers-color-scheme: dark)').addListener(e => {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/transpancy.css"><link rel="stylesheet" href="/css/fonts.css"><link rel="stylesheet" href="/css/title.css"><link rel="stylesheet" href="/css/flipcountdown.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous"><script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script><script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"></script><!-- hexo injector head_end start --><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/font-awesome-animation.min.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/tag_plugins.css" media="defer" onload="this.media='all'"><script src="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/carousel-touch.js"></script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.1.1"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">𝔸𝕣𝕔𝕙𝕚𝕧𝕖𝕤</div><div class="length-num">112</div></a><a href="/tags/"><div class="headline">𝕋𝕒𝕘𝕤</div><div class="length-num">35</div></a><a href="/categories/"><div class="headline">𝔽𝕠𝕝𝕕𝕖𝕣𝕤</div><div class="length-num">17</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page group hide" href="javascript:void(0);"><i class="fa-fw fas fa-book-open"></i><span> 筆記本</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/notesacm/"><i class="fa-fw fa fa-quran"></i><span> ACM</span></a></li><li><a class="site-page child" href="/notes/"><i class="fa-fw fa fa-tanakh"></i><span> 全部筆記</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/diary/"><i class="fa-fw fa fa-book"></i><span> 記事簿</span></a></div><div class="menus_item"><a class="site-page" href="/askbox/"><i class="fa-fw fa fa-comment"></i><span> 提問箱</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 好盆友</span></a></div><div class="menus_item"><a class="site-page group hide" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 更多</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/posts/c096c313.html"><i class="fa-fw fa fa-subway"></i><span> 地鐵圖</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-user"></i><span> 關於小明</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/index.png')"><nav id="nav"><span id="blog-info"><a href="/" title="小明の雜貨鋪"><span class="site-name">小明の雜貨鋪</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page group hide" href="javascript:void(0);"><i class="fa-fw fas fa-book-open"></i><span> 筆記本</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/notesacm/"><i class="fa-fw fa fa-quran"></i><span> ACM</span></a></li><li><a class="site-page child" href="/notes/"><i class="fa-fw fa fa-tanakh"></i><span> 全部筆記</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/diary/"><i class="fa-fw fa fa-book"></i><span> 記事簿</span></a></div><div class="menus_item"><a class="site-page" href="/askbox/"><i class="fa-fw fa fa-comment"></i><span> 提問箱</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 好盆友</span></a></div><div class="menus_item"><a class="site-page group hide" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 更多</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/posts/c096c313.html"><i class="fa-fw fa fa-subway"></i><span> 地鐵圖</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-user"></i><span> 關於小明</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">無標題</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label"></span><time class="post-meta-date-created" datetime="2024-05-31T09:51:59.802Z" title=" 2024-05-31 17:51:59">2024-05-31</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label"></span><time class="post-meta-date-updated" datetime="2024-06-14T04:53:50.589Z" title=" 2024-06-14 12:53:50">2024-06-14</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">總字數:</span><span class="word-count">17.2k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">預計閱讀時長:</span><span>67分鐘</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">閱讀量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p><br><br>不同教材的符号体系略有差别，本笔记以浙江大学编，高等教育出版社出版的《概率论与数理统计》为参考，有少许修改。</p>
<p>本笔记的例题均选自重要课后习题或本校期末考试真题。</p>
<p>本笔记的个人倾向性较强，</p>
<div style="page-break-after: always;"></div>


<h4 id="§4-3-3-协方差矩阵"><a href="#§4-3-3-协方差矩阵" class="headerlink" title="§4.3.3　协方差矩阵"></a>§4.3.3　协方差矩阵</h4><p>在很多情况下，我们实际上会需要关心一个随机向量的很多分量之间的协方差。因此，我们对协方差做一个推广，利用矩阵来简便地表示一个随机向量中任意两个分量之间的协方差。这就是协方差矩阵。我们先对向量与矩阵的数学期望进行定义，再引出协方差矩阵的定义。</p>
<p><strong>定义：</strong></p>
<p><strong>\pmb{X}=(X_1,\dots ,X_n)</strong></p>
<p><strong>则 $\pmb{\mu}=E\pmb{X}=(EX_1,\dots ,EX_n)$ 为随机向量的数学期望。</strong></p>
<p><strong>类似地，若 $Y=\begin{bmatrix} X<em>{11} &amp;\dots  &amp;X</em>{1n} \ \dots  &amp;\dots  &amp;\dots  \ X<em>{m1} &amp;\dots  &amp;X</em>{mn} \end{bmatrix}$ 为随机变量组成的矩阵</strong></p>
<p><strong>则 $EY=\begin{bmatrix} EX<em>{11} &amp;\dots  &amp;EX</em>{1n} \ \dots  &amp;\dots  &amp;\dots  \ EX<em>{m1} &amp;\dots  &amp;EX</em>{mn} \end{bmatrix}$ 为其数学期望。</strong></p>
<p>如此定义的数学期望，事实上，保留了数学期望的重要性质，下面进行验证。 \pmb{a} 为 n 维常数行向量， A,B 为 m\times n 常数矩阵。</p>
<script type="math/tex; mode=display">
E(\pmb{a}\pmb{X}^T)=E\sum_{j=1}^na_jX_j=\sum_{j=1}^na_jEX_j=\pmb{a}E\pmb{X}^T</script><p>同样地，我们有 $(E\pmb{Y})^T=E\pmb{Y}^T ， E(AYB)=A(EY)B$ ，由读者自己验证。</p>
<p>这告诉我们：如此进行推广后的数学期望仍旧<strong>保留了线性性</strong>。（注意到矩阵乘法实质上是线性表出）</p>
<p>我们接下来，定义协方差矩阵。</p>
<p><strong>定义：</strong></p>
<p>$\pmb{X}=(X_1,\dots ,X_n) ，\Sigma=E(\pmb{X}-\pmb{\mu})^T(\pmb{X}-\pmb{\mu})$ 称为 \pmb{X} 的协方差矩阵。</p>
<p>其中假设协方差矩阵第 i 行 j 列的元素为 \sigma<em>{ij} ，则我们不难发现 \sigma</em>{ij}=E(X_i-EX_i)(X_j-EX_j)=cov(X_i,X_j) 。这表明，协方差矩阵中的每一个元素都是随机向量某两个分量的协方差。且容易知道 ：协方差矩阵必定是对称矩阵。</p>
<p>关于协方差矩阵，我们简单地介绍其性质：</p>
<p><strong>1、 $\Sigma$ 非负定。</strong></p>
<p>证明：</p>
<p>要证明一个矩阵的非负定性，不妨将其化为二次型。</p>
<p>任意取行向量 \pmb{a}=(a_1,\dots ,a_n)</p>
<p>\pmb{a}\Sigma\pmb{a}^T=\sum<em>{i=1}^n\sum</em>{j=1}^na<em>i\sigma</em>{ij}a<em>j=E[\sum</em>{i=1}^n\sum_{j=1}^na_ia_j(X_i-\mu_i)(X_j-\mu_j)]</p>
<p>=E[\sum<em>{j=1}^na_j(X_j-\mu_j)]^2=Var[\sum</em>{j=1}^na_j(X_j-\mu_j)]\geqslant 0</p>
<p>得证。</p>
<p><strong>2、 \Sigma 退化当且仅当 X<em>1,X_2,\dots ,X_n 线性相关。（即 \exists a_1,\dots ,a_n 不全为零， \sum</em>{j=1}^na_j(X_j-EX_j)=0\ a.s. ）</strong></p>
<p>协方差矩阵退化当且仅当1中等号成立。回忆方差的性质： VarX=0 当且仅当 X=0\ a.s.</p>
<p><strong>四、条件数学期望</strong></p>
<p>在笔记（三）中，我们曾经介绍过条件概率分布。</p>
<p>在本章中，我们假设 (X,Y) 为连续型随机向量， f 为联合密度。离散型随机向量也可以类似处理。</p>
<p>回忆已知 Y=y 条件下 X 的条件密度 f_{X|Y}(x|y)=\frac{f(x,y)}{f_Y(y)} 。</p>
<p>那么结合数学期望的定义进行推广，我们不难知道 $E(X|Y=y)=\int<em>{-\infty}^{+\infty}xf</em>{X|Y}(x|y)\mathrm dx$  。</p>
<p>回忆在笔记（三）末尾我们提及的将条件概率值抽象为条件概率随机变量的方法。记 g(y)=E(X|Y=y) ，则 g(Y)=E(X|Y) 。 E(X|Y) 是一个与 Y 相关的随机变量，它反映了 y 取值变动时， E(X|Y=y) 会如何变动。</p>
<p>而 E(X|Y) 即称为 X 关于 Y 的条件数学期望。</p>
<p>这样定义所得的条件数学期望 E(X|Y) 关于 X具有数学期望所具备的一切性质。</p>
<p>我们先来研究条件数学期望的性质，再来关注它的诸多应用。</p>
<p>性质：</p>
<p><strong>1、（线性性） $\forall a<em>0,a_1,\dots ,a_n\in \mathbb{R},E(a_0+\sum</em>{i=1}^na<em>iX_i)=a_0+\sum</em>{i=1}^na_iEX_i</strong>$</p>
<p>**2、 $E[h(Y)g(X)|Y]=h(Y)E[g(X)|Y]$</p>
<p>证明：</p>
<p>E[h(Y)g(X)|Y=y]=E[h(y)g(X)|Y=y]=h(y)E[g(X)|Y=y] ，得证。</p>
<p>此性质表明：以 Y 为条件的条件期望中，任何 Y 的函数均可作为常数提出。</p>
<p><strong>3、 X,Y 独立，则 $E[g(X)|Y]=E[g(X)]$ 。</strong></p>
<p>**4、（全期望公式） $E[E(g(X)|Y)]=Eg(X)$</p>
<p>证明：</p>
<p>E[g(X)|Y=y]=\int<em>{\mathbb{R}}g(x)f</em>{X|Y}(x|y)\mathrm dx =\int _{\mathbb{R}}g(x)\frac{f(x,y)}{f_Y(y)}\mathrm dx </p>
<p>而 E(g(X)|Y=y) 实质上是 y 的函数，故对 E(g(X)|Y) 再求期望相当于对 Y 再求期望。</p>
<p>E[E(g(X)|Y)]=\int<em>{\mathbb{R}}E(g(X)|Y=y)f_Y(y)\mathrm dy=\iint</em>{\mathbb{R}^2}g(x)f(x,y)\mathrm dx \mathrm dy=E[g(X)]</p>
<p>全期望公式看似复杂，实际上在说一件很简单的事：期望与概率相似，也能够进行拆分再求和。例如： Y=y_1/y_2/y_3 一共有三个取值，那么 EX=P(Y=y_1)E(X|Y=y_1)+P(Y=y_2)E(X|Y=y_2)+P(Y=y_3)E(X|Y=y_3)</p>
<p>事实上，<strong>全期望公式就是期望版的全概公式</strong>。</p>
<p>此处不妨多提一句：方差也能够如此进行推广，得到条件方差 Var(X|Y) 。</p>
<p>显然地， Var(X|Y)=E(X^2|Y)-E^2(X|Y) 。</p>
<p>然而，<strong>条件方差可没有对应的“全方差公式”</strong>。</p>
<p>我们在之后谈及概率论应用中涉及到混合分布时会深入了解到其原因。</p>
<p>条件数学期望的应用非常广泛。首先，<strong>条件数学期望经常用来解决一些条件分布以及期望的求解问题。很多情况下，通过全期望公式求解期望远比先求得分布再求出期望要简单。</strong>我们直接通过一个经典的例子来阐述这种方法。</p>
<p>例：超时某日的顾客总数为 N,N\sim P(\lambda) ，顾客之间消费额独立同分布，且与 N 独立。 S 为全天营业额。顾客平均消费为 \mu ，求该日平均营业额。</p>
<p>解：</p>
<p>X_i 为第 i 个客人的消费额，则 S=X_1+X_2+\dots +X_N ， EX_i=\mu 。</p>
<p>然而难题是： S 与 X_i 有关，而 X_i 的个数又取决于 N ，这个时候我们自然地联想到研究 N 为固定值的条件下 S 的条件分布。</p>
<p>E(S|N=n)=E(X_1+\dots +X_n|N=n)</p>
<p>由于顾客消费额与顾客人数独立，利用条件数学期望的性质， E(S|N=n)=E(X_1+\dots +X_n)=n\mu</p>
<p>E(S|N)=N\mu</p>
<p>再利用全期望公式， ES=E[E(S|N)]=E(N\mu)=\mu EN=\mu\lambda 。</p>
<p>这个例子其实证明了一个直觉上很显然的事实：在这样的条件下，平均顾客数乘以每个人的平均消费额等于平均营业额。</p>
<p>条件数学期望的理论应用是更加有趣的：</p>
<p>事实上，<strong>条件数学期望 E(X|Y) 是所有用 Y 的函数对 X 进行预测中均方差意义下最优的</strong>。读者可能现在完全不能理解这句话的意思，我们先从简单的意义开始深入介绍。</p>
<p>我们先来看条件期望的一个性质：</p>
<p>若 EX^2&lt;\infty,Eh^2(Y)&lt;\infty ， h 为实函数，则 E[(X-E(X|Y))h(Y)]=0 。</p>
<p>证明：</p>
<p>我们想对上式进行线性拆分，则先需要证明数学期望 E(Xh(Y)) 的存在性。</p>
<p>由内积不等式， E|Xh(Y)|\leqslant \sqrt{EX^2Eh^2(Y)}&lt;\infty</p>
<p>则 E[(X-E(X|Y))h(Y)]=E[Xh(Y)]-E[E(Xh(Y)|Y)]</p>
<p>由全期望公式， =E[Xh(Y)]-E[Xh(Y)]=0</p>
<p>得证。</p>
<p>我们现在来解释这条性质的意义。</p>
<p>回忆我们在之前内积不等式处证明中提及的， EXY 可以作为 X,Y 的一个内积，从而形成线性空间。我们现在对这个线性空间进行一些拓展。</p>
<p>\sqrt{EX^2} 即为 X 代表的向量在此空间中的长度。</p>
<p>我们翻译一下上述条件数学期望的性质： X （代表的向量）长度有限且 h(Y) 长度有限时， X-E(X|Y) 总是与 h(Y) 正交。</p>
<p>我们突然发现：<strong>条件数学期望 E(X|Y) 的实质是 X 在平面</strong> h(Y) <strong>上的正交投影。</strong>这是非常重要的性质，这意味着我们在线性空间中对正交投影的性质都可以推广到条件数学期望上。</p>
<p>从<strong>线性空间的角度去看待条件数学期望</strong>，我们可以得出一大堆条件数学期望的性质。</p>
<p>如：<strong>勾股定理：</strong> E[E^2(X|Y)]+E[(X-E(X|Y))^2]=EX^2</p>
<p><strong>直角三角形斜边大于直角边：</strong> EX^2\geqslant E[X-E(X|Y)]^2,EX^2\geqslant E(E^2(X|Y))</p>
<p>我们下面介绍为何条件数学期望是最佳的预测。</p>
<p>先给出定理，然后我们分别从线性空间的角度与代数的角度去证明其正确性。</p>
<p><strong>定理（最佳预测）</strong></p>
<p><strong>EX^2&lt;\infty,g(y) 为任意实函数，则 E[X-E(X|Y)]^2\leqslant E[X-g(Y)]^2 。等号成立当且仅当</strong> g(Y)=E(X|Y)\ a.s.</p>
<p>证1：</p>
<p>从线性空间的角度去看，左侧即为 X 到 g(Y) 平面的最短正交距离的平方。右侧为 X 到 g(Y) 平面上某一点的距离的平方。显然地，正交距离在欧氏距离意义下是所有距离中最短的，定理得证。</p>
<p>证2：</p>
<p>从代数计算的角度看问题。</p>
<p>需要讨论 Eg^2(Y) 是否有限。</p>
<p>若 Eg^2(Y)=\infty ， Eg^2(Y)\leqslant 2E[X-g(Y)]^2+2EX^2,E[X-g(Y)]^2=\infty ，得证。</p>
<p>若 Eg^2(Y)&lt;\infty ，由内积不等式， E^2(X|Y=y)\leqslant E(X^2|Y=y) ，则 E^2(X|Y)\leqslant E(X^2|Y) 。</p>
<p>故 E[E^2(X|Y)]\leqslant EX^2&lt;\infty ， E[E(X|Y)-g(Y)]^2&lt;\infty 。</p>
<p>做完了预备工作，我们进入正题。</p>
<p>E[X-g(Y)]^2=E[X-E(X|Y)+E(X|Y)-g(Y)]^2</p>
<p>=E[X-E(X|Y)]^2+E[E(X|Y)-g(Y)]^2+2E[(X-E(X|Y))(E(X|Y)-g(Y))]</p>
<p>由前面例题中介绍的条件期望的性质， E[(X-E(X|Y))(E(X|Y)-g(Y))]=0</p>
<p>故原式 =E[X-E(X|Y)]^2+E[E(X|Y)-g(Y)]^2\geqslant E[X-E(X|Y)]^2</p>
<p>当且仅当 g(Y)=E(X|Y) 时，等号成立。</p>
<p>我们可以清晰地感受到利用条件数学期望在线性空间中的性质可以直观而又简便地得到很强的结论。（利用我们对线性空间性质已有的透彻研究）</p>
<p><strong>最佳预测性质告诉我们：我们现在想利用随机变量 Y 的函数去拟合逼近随机变量 X ，那么 E(X|Y) 在均方差最小的意义下（残差平方和的平均最小）是最优的。</strong></p>
<p>这个性质事实上在数理统计中有着非常广泛的应用。</p>
<p>我们下举一例让读者进行体会：</p>
<p>例： (X,Y)\sim N(\mu_1,\mu_2,\sigma_1^2,\sigma_2^2,\rho) ，我们通过计算可以求得 E(X|Y)=\mu_1+\rho\frac{\sigma_1}{\sigma_2}(Y-\mu_2) 。</p>
<p>有趣的是： E(X|Y) 是 Y 的线性函数。所以此时，最佳预测的问题就转化为了最佳线性预测的问题。</p>
<p>这告诉我们：<strong>对正态分布而言，最佳预测就是最佳线性预测。事实上，与后文要提到的中心极限定理相结合，我们就能理解之所以线性回归被如此广泛应用的原因。</strong>（在我高中自学统计时，学习到线性回归时一度以为这是前人为了简化回归模型而应用线性的。事实上，很多方法真的不是拍脑袋想出来的，而是有确凿的数学推断与原因的。我认为真正的逻辑是：独立同分布的随机变量的均值选择了正态分布，而正态分布选择了线性模型作为其最佳预测。</p>
<p>最后，我们不加证明地给出一个求条件数学期望 E(X|A) 的公式。</p>
<p><strong>定理：</strong></p>
<p><strong>A 为事件， P(A)&gt;0,E(X|A) 存在，则 E(X|A)=\frac{E(XI[A])}{P(A)} 。</strong></p>
<p>我们举一例阐释其如何应用。</p>
<p>例：</p>
<p>X\sim \varepsilon(\lambda),\forall a&gt;0, 证明 E(X-a|X&gt;a)=EX 。</p>
<p>证明：</p>
<p>E(X|X&gt;a)=\frac{E(XI[X&gt;a])}{P(X&gt;a)}=\frac{\int_a^{+\infty}x\lambda e^{-\lambda x}\mathrm dx }{e^{-\lambda a}}</p>
<p>=\frac{1}{\lambda}+a=EX+a</p>
<p>事实上，<strong>回忆指数分布的无记忆性。</strong></p>
<p><strong>我们容易知道：若 X\sim \varepsilon(\lambda) ，X-a|_{X&gt;a} 与 X 是同分布的。</strong></p>
<p>这个例子与指数分布的无记忆性是统一的。</p>
<h1 id="概率论学习笔记（五）"><a href="#概率论学习笔记（五）" class="headerlink" title="概率论学习笔记（五）"></a>概率论学习笔记（五）</h1><p>终于迎来了第五篇笔记。在这篇笔记中，我们将会讨论一些概率论中十分重要的定理。我们会先介绍几类概率论常用函数，紧接着用这些函数来解决诸如 n 维正态分布、大数律、中心极限定理等问题。特别地，我们将会给出中心极限定理的特征函数法证明，读者将会感受到正态分布之所以在现实生活中如此常见的原因。与此相伴地，本章内容可能比较艰深，需要读者自行多加体会。</p>
<p>我们先介绍三种概率论中常用的函数。这三种函数都是为了方便地计算独立随机变量的和的分布而被引入的。</p>
<p><strong>一、概率母函数</strong></p>
<p><strong>定义：</strong></p>
<p><strong>X 为非负，取整数值的离散型随机变量。 s\in [-1,1] ， g(s)=Es^X 称为 X 的概率母函数。</strong></p>
<p>若将概率母函数的表达式展开，我们可以得到 g(s)=\sum_{j=0}^{\infty}s^jP(X=j) ，故 g(s) 在 [-1,1] 上是绝对收敛的。</p>
<p>那么为何这个函数叫作概率母函数呢？事实上，由概率母函数我们可以把这个分布唯一地确定下来。</p>
<p>我们来看一些其重要的性质：</p>
<p><strong>1、 \forall k\in \mathbb{N},\ P(X=k)=\frac{g^{(k)}(0)}{k!}</strong></p>
<p>证明：</p>
<p>容易看出 g(s) 及其导数级数均在 [-1,1] 中内闭一致收敛，故可逐项求导。（事实上，可以证明 g(s) 可以无穷次逐项求导）</p>
<p>\frac{g^{(k)}(s)}{k!}=\frac{1}{k!}\sum_{j=k}^{\infty}\frac{d^ks^j}{ds^k}P(X=j)</p>
<p>故可得 \frac{g^{(k)}(0)}{k!}=P(X=k) 。</p>
<p><strong>2、 EX=g^{‘}(1)</strong></p>
<p>证明：</p>
<p>g^{‘}(1)=\sum_{j=1}^{\infty}jP(X=j)=EX</p>
<p><strong>3、若 X<em>1,\dots ,X_n 相互独立， g_i(s) 为 X_i 的概率母函数，则 Y=\sum</em>{j=1}^nX<em>j 有概率母函数 g_Y(s)=\prod</em>{j=1}^ng_j(s) 。</strong></p>
<p>证明：</p>
<p>由于X_1,\dots ,X_n 相互独立，容易知道 s^{X_1},\dots ,s^{X_n} 相互独立。</p>
<p>由数学期望的性质， g<em>Y(s)=Es^{\sum</em>{j=1}^nX<em>j}=\prod</em>{j=1}^nEs^{X<em>j}=\prod</em>{j=1}^ng_i(s)</p>
<p>这三个性质都是很有用的。性质1告诉我们：<strong>概率母函数与概率分布是相互唯一决定的。</strong>概率母函数的各阶导数在 0 处的取值对应着概率。</p>
<p>对于性质2，实际上各阶原点矩都是可以求出的。读者可以自行验证 EX^2=g^{‘’}(1)+g^{‘}(1) 。要求 n 阶中心矩，只需要考虑 n 阶导数在 1 处的取值即可。</p>
<p>对于性质3，则是最重要的性质。<strong>它为我们提供了一种手段研究独立随机变量之和：先求出随机变量之和的概率母函数，再由概率母函数与概率分布的一一对应性反过来研究概率分布。</strong>这样的性质与思想也会贯穿后面我们要提到的矩母函数以及特征函数。</p>
<p>我们接下来，给出常用离散分布的概率母函数，并且介绍概率母函数的应用。</p>
<p>常用离散分布的概率母函数，请读者自行计算：</p>
<p>1、 X\sim B(n,p),g(s)=(1-p+sp)^n</p>
<p>2、 X\sim P(\lambda),g(s)=e^{\lambda(s-1)}</p>
<p>3、 X\sim G(p),g(s)=\frac{sp}{1-s(1-p)}</p>
<p>4、 X\sim Pas(r,p),g(s)=(\frac{sp}{1-s(1-p)})^r</p>
<p><strong>概率母函数的一个重要应用就是证明概率分布关于某些参数具有独立可加性。</strong></p>
<p>我们举例说明这一点：</p>
<p>例<strong>（二项分布关于 n 的独立可加性）</strong></p>
<p>X_1,\dots ,X_k,\ X_i\sim B(n_i,p) ，均相互独立。</p>
<p>则 Y=\sum<em>{j=1}^kX_j,g_Y(s)=\prod</em>{j=1}^kg_i(s)</p>
<p>求得 g<em>Y(s)=(1-p+sp)^{\sum</em>{i=1}^kn_i}</p>
<p>我们发现 B(\sum_{i=1}^kn_i,p) 的概率母函数与 Y 的概率母函数完全相同。</p>
<p>由概率母函数与概率分布一一对应， Y\sim B(\sum_{i=1}^kn_i,p) 。</p>
<p>我们再举一个稍复杂点的例子。细心的读者一定注意到了帕斯卡分布的概率母函数是 r 个几何分布的概率母函数的乘积。那我们不妨证明如下命题：</p>
<p>例： X<em>1,\dots ,X_r,X_i\sim G(p) ，均相互独立，证明： Y=\sum</em>{i=1}^rX_i\sim Pas(r,p)</p>
<p>证明：</p>
<p>由概率母函数的性质3，容易求出 g_Y(s)=(\frac{sp}{1-s(1-p)})^r 。</p>
<p>进行Taylor展开，得到 g<em>Y(s)=(sp)^r\sum</em>{j=0}^\infty \binom{r+j-1}{r-1}(s-sp)^j=\sum_{k=r}^\infty\binom{k-1}{r-1}p^r(1-p)^{k-r}s^k</p>
<p>我们容易看出，概率母函数经过变形后有了 \sum_{j=0}^\infty P(X=j)s^j 的形式。那么同样地，利用概率母函数与概率分布一一对应的性质，我们知道 P(Y=k)=\binom{k-1}{r-1}p^r(q-p)^{k-r}\ (k=r,r+1,\dots ) ，为帕斯卡分布 Pas(r,p) 。</p>
<p>在此我列出了<strong>常用离散分布关于某些参数具有的独立可加性</strong>，读者可以自己用概率母函数进行验证：</p>
<p>1、 B(n,p) 关于 n</p>
<p>2、 P(\lambda) 关于 \lambda</p>
<p>3、 Pas(r,p) 关于 r</p>
<p>概率母函数的另一个应用便是<strong>在计算繁琐的概率时往往能够提供便利</strong>。</p>
<p>我们以例子阐释方法。</p>
<p>例：投掷 n 颗骰子，求总点数为 k=n+6 的概率。</p>
<p>解：</p>
<p>X<em>1,\dots ,X_n 分别为第 i 颗骰子的点数，相互独立且同分布。 Y=\sum</em>{i=1}^nX_i 。</p>
<p>g_{X_i}(s)=Es^{X_i}=\frac{1}{6}(s+s^2+\dots +s^6)=\frac{s(1-s^6)}{6(1-s)}</p>
<p>故由概率母函数的性质， g_Y(s)=\frac{s^n(1-s^6)^n}{6^n(1-s)^n}</p>
<p>回忆概率母函数的性质， P(Y=k)=\frac{g_Y^{(k)}(0)}{k!} ，即为 g_Y(s) 在 0 处Taylor展开式中 s^k 的系数。</p>
<p>对概率母函数进行Taylor展开， g<em>Y(s)=\frac{s^n(1-s^6)^n}{6^n}\sum</em>{i=0}^\infty\binom{i+n-1}{n-1}s^i</p>
<p>可以求出 s^{n+6} 项的系数为 \frac{\binom{n+5}{n-1}-n}{6^n} ，即为所求概率。</p>
<p><strong>二、特征函数</strong></p>
<p>对于特征函数，我们可能要利用到一些复数的知识。</p>
<p><strong>定义：</strong></p>
<p><strong>若 X,Y 为实随机变量， i 为虚数单位， Z=X+iY 。若 EX,EY 存在，则 EZ=EX+iEY 。</strong></p>
<p><strong>对于随机变量 X ， \phi(t)=Ee^{itX},t\in \mathbb{R} 称为 X 的特征函数。</strong></p>
<p>容易知道，对于离散型随机变量， \phi(t)=\sum<em>ke^{itk}P(X=x_k) 。对于连续型随机变量，假设密度为 f ，\phi(t)=\int</em>{\mathbb{R}}e^{itx}f(x)\mathrm dx  。</p>
<p>对于概率母函数，我们知道其与概率分布唯一地相互对应。那么问题是：特征函数是否与概率分布唯一地相互对应呢？答案是肯定的。我们由特征函数的定义知道给定了概率分布一定能计算出特征函数，下面我们介绍由特征函数如何反推出概率分布。</p>
<p>我们不加证明地给出逆转公式。</p>
<p><strong>定理（逆转公式）</strong></p>
<p><strong>\phi(t) 为 X 的特征函数， F 为分布函数，则对于任意的 F 的连续点 a,b ， F(b)-F(a)=\frac{1}{2\pi}\lim<em>{T\rightarrow\infty}\int</em>{-T}^T\frac{e^{-ita}-e^{-itb}}{it}\phi(t)dt 。</strong></p>
<p>逆转公式的本质是一个Fourier变换，有兴趣的读者可以自行查阅其证明。</p>
<p>事实上，对逆转公式还能进行一下推广。更一般地， \forall a,b （不必是连续点）， \frac{1}{2\pi}\lim<em>{T\rightarrow\infty}\int</em>{-T}^T\frac{e^{-ita}-e^{-itb}}{it}\phi(t)dt=P(a&lt;X&lt;b)+\frac{P(X=a)+P(X=b)}{2}</p>
<p>可以得到 \frac{1}{2\pi}\lim<em>{T\rightarrow\infty}\int</em>{-T}^T\frac{e^{-ita}-e^{-itb}}{it}\phi(t)dt=\frac{F(b)-F(a)+F(b-0)-F(a-0)}{2} 。</p>
<p>我们给出逆转公式的目的事实上是为了说明：由特征函数也可以唯一地确定概率分布。特征函数与概率分布是唯一地相互决定的。</p>
<p>关于特征函数的性质，我们给出如下这几点：（均仅对连续型随机变量的情形证明）</p>
<p><strong>1、 \phi(0)=1,|\phi(t)|\leqslant 1,\overline{\phi(t)}=\phi(-t)</strong></p>
<p>证明：</p>
<p>由内积不等式与Euler公式， |\phi(t)|=|EcostX+iEsintX|\leqslant \sqrt{|Ecos^2tX+Esin^2tX|}=1</p>
<p>同样地，由Euler公式， \overline{\phi(t)}=EcostX-iEsintX=\phi(-t) 。</p>
<p><strong>2、 \phi(t) 在 \mathbb{R} 上一致连续。</strong></p>
<p>证明：</p>
<p>要证明一致连续性，我们只需要对 |\phi(t)-\phi(s)| 进行估计。</p>
<p>容易知道 |\phi(t)-\phi(s)|=|E(e^{itX}-e^{isX})|=|\int_{\mathbb{R}}(e^{itX}-e^{isX})f(x)\mathrm dx |</p>
<p>那么我们的任务转变为了对这个定积分进行估计。</p>
<p>而 |\int<em>{\mathbb{R}}(e^{itX}-e^{isX})f(x)\mathrm dx |\leqslant\int</em>{\mathbb{R}}|e^{itX}-e^{isX}|f(x)\mathrm dx </p>
<p>对于后面的定积分，我们注意到：当 x 的绝对值很大时，我们可以利用 f(x) 无穷积分的收敛性进行估计。而当 x 的绝对值较小时，我们可以利用 e^{ix} 的连续型进行估计。故我们将积分分为两部分。</p>
<p>\int<em>{\mathbb{R}}|e^{itX}-e^{isX}|f(x)\mathrm dx =\int</em>{|x|\leqslant M}|e^{i(t-s)X}-1|f(x)\mathrm dx +\int_{|x|&gt;M}|e^{i(t-s)X}-1|f(x)\mathrm dx </p>
<p>对于 \int_{|x|\leqslant M}|e^{i(t-s)X}-1|f(x)\mathrm dx  ， \forall \epsilon&gt;0,\exists \delta&gt;0,\ s.t.\ |x|\leqslant \delta 时， |e^{-ixM}-1|&lt;\frac{\epsilon}{2} 。</p>
<p>另一方面，由 f 无穷积分收敛的Cauchy准则， \forall \epsilon&gt;0,\exists M,s.t.\ \int_{|x|&gt;M}f(x)\mathrm dx &lt;\frac{\epsilon}{2}</p>
<p>故综上所述， \forall \epsilon&gt;0,\exists M&gt;0,\delta&gt;0,s.t.\ |s-t|&lt;\delta 时，有 |\phi(s)-\phi(t)|&lt;\epsilon 成立，得证。</p>
<p><strong>3、 若EX^k 存在， \phi^{(k)}(t)=i^kE(X^ke^{itX})</strong></p>
<p>证明：</p>
<p>容易证明含参变量无穷积分 \int_{-\infty}^\infty e^{itx}f(x)\mathrm dx  对 t\in \mathbb{R} 一致收敛，可证明求导能够与积分交换顺序。</p>
<p>得到 \phi^{(k)}(t)=\int<em>{\mathbb{R}}\frac{d^k}{dt^k}e^{itx}f(x)\mathrm dx =i^k\int</em>{\mathbb{R}}x^ke^{itx}f(x)\mathrm dx =i^kE(X^ke^{itX})</p>
<p><strong>一个重要的推论是： \phi^{(k)}(0)=i^kEX^k 。</strong></p>
<p><strong>4、若 X<em>i 相互独立，分别有特征函数 \phi_i(t) ，则 Y=\sum</em>{i=1}^nX<em>i 有特征函数 \phi_Y(t)=\prod</em>{i=1}^n\phi_i(t) 。</strong></p>
<p>类似地，性质4是核心的性质。是我们利用特征函数来研究独立随机变量之和的分布的便利所在。我们接下来给出常见分布的特征函数，并且阐释其应用。</p>
<p><strong>常用分布的特征函数</strong>，请读者自己验证：</p>
<p>1、 X\sim B(n,p),\phi(t)=(1-p+pe^{it})^n</p>
<p>2、 X\sim P(\lambda),\phi(t)=e^{\lambda(e^{it}-1)}</p>
<p>3、 X\sim G(p),\phi(t)=\frac{pe^{it}}{1-(1-p)e^{it}}</p>
<p>4、 X\sim Pas(r,p),\phi(t)=(\frac{pe^{it}}{1-(1-p)e^{it}})^r</p>
<p>5、 X\sim U(a,b),\phi(t)=\frac{e^{ibt}-e^{iat}}{it(b-a)}</p>
<p>6、 X\sim \varepsilon(\lambda),\phi(t)=(1-\frac{it}{\lambda})^{-1}</p>
<p>7、 X\sim \Gamma(\alpha,\beta),\phi(t)=(1-\frac{it}{\beta})^{-\alpha}</p>
<p>8、 X\sim N(\mu,\sigma^2),\phi(t)=e^{i\mu t-\frac{\sigma^2t^2}{2}}</p>
<p>其中，我们将给出正态分布的特征函数的计算过程，因为这是特别重要的。</p>
<p>例<strong>（正态分布的特征函数）</strong></p>
<p>对于正态分布直接上手求 N(\mu,\sigma^2) 是困难的。不妨先求出标准正态分布的特征函数，再反推出一般正态分布的特征函数。</p>
<p>X\sim N(0,1) ， \phi(t)=Ee^{itX}=\frac{1}{\sqrt{2\pi}}\int_{\mathbb{R}}e^{itx-\frac{x^2}{2}}\mathrm dx </p>
<p>对于这样类型的积分，自然的做法就是对指数进行配方。</p>
<p>得到 \frac{1}{\sqrt{2\pi}}e^{-\frac{t^2}{2}}\int_{\mathbb{R}}e^{-\frac{(x-it)^2}{2}}\mathrm dx =e^{-\frac{t^2}{2}} 。</p>
<p>此处值得一提的是，这只是一种形式地做法。（真正严谨的做法要利用到微分方程 \phi^{‘}(t)=-t\phi(t) 以及初值条件 \phi(0)=1 来解出）</p>
<p>那么对于 Y\sim N(\mu,\sigma^2) ，容易知道 \frac{Y-\mu}{\sigma}\sim N(0,1)</p>
<p>故 \phi_Y(t)=Ee^{itY}=Ee^{it(\mu+\sigma X)}=e^{it\mu}\phi_X(\sigma t)=e^{it\mu-\frac{\sigma^2t^2}{2}} 。</p>
<p>关于特征函数的应用，与概率母函数的类似，可以<strong>证明概率分布关于某个参数具有独立可加性</strong>。（事实上，离散型分布均可用概率母函数处理，只有连续型随机变量需要利用特征函数处理）</p>
<p>下面给出常用连续型分布关于参数的独立可加性，读者可以模仿概率母函数处的证明利用特征函数性质4自行证明：</p>
<p>1、 N(\mu,\sigma^2) 关于 \mu,\sigma^2</p>
<p>2、 \Gamma(\alpha,\beta) 关于 \alpha</p>
<p>最后，在这一章我们为后面要介绍的大数律以及中心极限定理进行一些准备工作，引出一些定义。这些定义都是关于一列随机变量的收敛性的。</p>
<p><strong>定义：</strong></p>
<p><strong>F 为 X 的 分布函数， F<em>n 为 X_n 的分布函数，若 x 为 F(x) 连续点，且 \lim</em>{n\rightarrow\infty}F_n(x)=F(x) ，则称 X_n 依分布收敛到 X ,记 X_n\overset{d}{\rightarrow}X 。</strong></p>
<p>容易看到依分布收敛意味着当 n 充分大时，我们能用 X 的分布近似为 X_n 的分布。</p>
<p>我们接下来不加证明地给出连续性定理。</p>
<p><strong>定理（连续性定理）</strong></p>
<p><strong>\phi<em>n(t) 为 X_n 特征函数，\phi(t) 为 X 特征函数。 X_n\overset{d}{\rightarrow}X 当且仅当 \forall t,\lim</em>{n\rightarrow\infty}\phi_n(t)=\phi(t) 。</strong></p>
<p>我们将会在中心极限定理的证明中利用到连续性定理。</p>
<p>对于随机向量，其特征函数可由一维情况推广而来。</p>
<p><strong>\pmb{X}=(X_1,\dots ,X_n) 为随机向量， \pmb{t}=(x_1,\dots ,x_n) ，则特征函数 \phi(\pmb{t})=Ee^{i\pmb{t}\pmb{X^T}} 。</strong></p>
<p><strong>三、多元正态分布</strong></p>
<p>在本章中，我们将用特征函数、矩阵来研究多元正态分布的性质，给出其密度。</p>
<p><strong>定义：</strong></p>
<p><strong>\pmb{\mu}=(\mu<em>1,\dots ,\mu_n)^T,B</em>{n\times m} 为常数矩阵， \epsilon_1,\dots ,\epsilon_m 为相互独立且服从标准正态分布的随机变量，\pmb{\epsilon}=(\epsilon_1,\dots ,\epsilon_m)^T ， \pmb{X}=(X_1,\dots ,X_n)^T 。</strong></p>
<p><strong>若 \pmb{X}=\pmb{\mu}+B\pmb{\epsilon} ，称 \pmb{X} 服从 n 维正态分布，记 X\sim N(\pmb{\mu},BB^T) 。</strong></p>
<p>读者容易发现，由于\epsilon_1,\dots ,\epsilon_m 为相互独立且服从标准正态分布的随机变量，其协方差矩阵 E\pmb{\epsilon}\pmb{\epsilon}^T=I 为单位矩阵。</p>
<p>所以我们的可以得到： E\pmb{X}=\pmb{\mu},\Sigma=E(\pmb{X}-\pmb{\mu})(\pmb{X}-\pmb{\mu})^T=BB^T 。这就是多元正态分布被记作 N(\pmb{\mu},BB^T) 的理由。</p>
<p>我们接下来来求得<strong>多元正态分布的特征函数</strong>。</p>
<p>容易知道 \phi_{\epsilon}(\pmb{t})=Ee^{i\pmb{t}^T\pmb{\epsilon}}=e^{-\frac{\pmb{t}^T\pmb{t}}{2}}</p>
<p>利用同样的手法，计算 \pmb{X} 的特征函数，得到 \phi_X(\pmb{t})=Ee^{i\pmb{t}^T\pmb{X}}=e^{i\pmb{t}^T\pmb{\mu}}Ee^{i\pmb{t}^TB\pmb{\epsilon}}</p>
<p>=e^{i\pmb{t}^T\pmb{\mu}-\frac{\pmb{t}^TBB^T\pmb{t}}{2}}=e^{i\pmb{t}^T\pmb{\mu}-\frac{\pmb{t}^T\Sigma\pmb{t}}{2}}</p>
<p>关于多元正态分布的性质，在此多数都直接给出结论，读者可以尝试自己证明。</p>
<p><strong>多元正态分布的性质：</strong></p>
<p><strong>1、 \pmb{X}=(X_1,\dots ,X_n)^T\sim N(\pmb{\mu},\Sigma) 当且仅当 \forall \pmb{a}=(a_1,\dots ,a_n)^T,\pmb{a}^T\pmb{X}\sim N(\pmb{a}^T\pmb{\mu},\pmb{a}^T\Sigma\pmb{a}) 。</strong></p>
<p><strong>（利用特征函数进行变形即可）</strong></p>
<p><strong>2、 \pmb{X}\sim N(\pmb{\mu},\Sigma), 则 X_1,\dots ,X_n 相互独立当且仅当 \Sigma=diag\left {\sigma_1^2,\dots ,\sigma_n^2 \right } 为对角矩阵。</strong></p>
<p><strong>3、当 \Sigma 正定时， \pmb{X} 为连续型随机向量，联合密度存在。</strong></p>
<p><strong>联合密度： f(x)=\frac{1}{(2\pi)^{\frac{n}{2}}\sqrt{det(\Sigma)}}e^{-\frac{(\pmb{x}-\pmb{\mu})^T\Sigma^{-1}(\pmb{x}-\pmb{\mu}) }{2}}</strong></p>
<p>接下来，我们介绍一种新的常用分布：卡方分布。</p>
<p><strong>定义： X<em>1,\dots ,X_n 独立同分布，均服从标准正态分布，则 Z=\sum</em>{i=1}^nX_i^2 具有概率密度 f_n(z)=\frac{1}{2^{\frac{n}{2}}\Gamma(\frac{n}{2})}z^{\frac{n}{2}-1}e^{-\frac{z}{2}}(z\geqslant 0) ，称 Z 服从自由度为 n 的卡方分布， Z\sim \chi^2(n) 。</strong></p>
<p>细心的读者会发现：<strong>\chi^2(n) 与 \Gamma(\frac{n}{2},\frac{1}{2}) 是等价的。</strong></p>
<p>关于其证明，传统的方法是利用数学归纳法，证明密度的递推关系是正确的。</p>
<p>我们在这里利用<strong>特征函数</strong>进行处理，以显示其威力。</p>
<p>证明：</p>
<p>设 Y_1=X_1^2 ，利用分布函数法求随机变量的函数的分布，可以容易地求得 Y_1\sim \Gamma(\frac{1}{2},\frac{1}{2}) 。（交给读者自己验证）</p>
<p>则 Z=\sum_{i=1}^nY_i ，且 Y_i 独立同分布。</p>
<p>这样的独立的随机变量之和的形式利用特征函数处理再简单不过。</p>
<p>回忆我们之前曾经求得过 X\sim \Gamma(\alpha,\beta),\phi(t)=(1-\frac{it}{\beta})^{-\alpha}。</p>
<p>\phi_Y(t)=(1-2it)^{-\frac{1}{2}} ，则 \phi_Z(t)=(1-2it)^{-\frac{n}{2}}</p>
<p>由特征函数与概率分布相互唯一决定， Z\sim \Gamma(\frac{n}{2},\frac{1}{2}) ，得证。</p>
<p>我们可以发现，证明过程事实上就是在证明伽马分布对于第一个参数的独立可加性。</p>
<p>最后，我们通过例子给出一个数理统计中的有趣结果。</p>
<p>例<strong>（样本均值与样本方差）</strong></p>
<p><strong>X<em>1,\dots ,X_n 独立同分布， \overline{X_n}=\frac{\sum</em>{i=1}^nX<em>i}{n} 为样本均值， s^2=\frac{\sum</em>{i=1}^n(X_i-\overline{X_n})^2}{n-1} 为样本方差。证明： X_i 服从标准正态分布时， \overline{X_n} 与 s^2 独立，且 (n-1)s^2\sim \chi^2(n-1) 。</strong></p>
<p>证明：</p>
<p>我们不妨定义正交矩阵 T=\frac{1}{\sqrt{n}}\begin{bmatrix} 1 &amp; 1 &amp; \dots  &amp;1 \ \dots &amp; \dots &amp;\dots  &amp; \dots \ \dots &amp;\dots  &amp; \dots &amp;\dots  \end{bmatrix} 。（我们只关心第一行均为1，这样经过矩阵乘法后可以得到 X_1+\dots +X_n ）</p>
<p>\pmb{X}=(X_1,\dots ,X_n)\sim N(\pmb{0},I)</p>
<p>则 \pmb{Y}=T\pmb{X}\sim N(\pmb{0},I)</p>
<p>我们知道了变换后的 Y_1,\dots ,Y_n 独立同分布且均服从标准正态分布，且 Y_1=\sqrt{n}\overline{X_n} 。</p>
<p>我们成功地利用正交矩阵的变换将样本均值变为了一个随机变量的同时，保持了其余分量独立同分布，均服从标准正态分布。</p>
<p>又注意到，正交变换还是保持度量的，故我们有 \sum<em>{i=1}^nX_i^2=\sum</em>{i=1}^nY_i^2 。</p>
<p>则 (n-1)s^2=\sum<em>{i=1}^nX_i^2-n\overline{X_n}^2=\sum</em>{i=1}^nY<em>i^2-Y_1^2=\sum</em>{i=2}^nY_i^2\sim\chi^2(n-1)</p>
<p>而且 (n-1)s^2 为 Y_2,\dots ,Y_n 的函数，且 Y_i 相互独立。故其与 Y_1 独立。</p>
<p>所以样本均值与样本方差独立 。</p>
<p>这个例子告诉我们十分有趣的一个结论：若样本是独立同为正态分布，样本均值竟然与样本方差互不影响。而且我们从这个结论其实已能得到样本方差的无偏性结论。（ Es^2=\sigma^2 ）</p>
<p><strong>四、大数律</strong></p>
<p>大数律主要研究的是当满足一定条件的随机变量 X<em>1,\dots ,X_n 的个数足够多 (n\rightarrow\infty) 时，这些随机变量的均值 \frac{S_n}{n}(S_n=\sum</em>{i=1}^nX_i) 将会如何收敛到其数学期望。</p>
<p>在进入正题之前，我们先介绍一会儿将会用到的两种随机变量序列的收敛性。</p>
<p><strong>定义：</strong></p>
<p><strong>\xi<em>n 为一列随机变量， \xi 为随机变量，若 \forall \epsilon&gt;0,\lim</em>{n\rightarrow\infty}P(|\xi_n-\xi|\geqslant \epsilon)=0 ，称 \xi_n 依概率收敛到 \xi ，记 \xi_n\overset{p}{\rightarrow}\xi 。</strong></p>
<p><strong>定义：</strong></p>
<p><strong>\xi<em>n 为一列随机变量， \xi 为随机变量，若 P(\lim</em>{n\rightarrow \infty}\xi_n=\xi)=1 ，称 \xi_n 几乎处处收敛到 \xi ，记 \xi_n\overset{a.s.}{\rightarrow}\xi 。</strong></p>
<p>读者现在可能还体会不到这两种收敛性之间的差别。我们在后面还将对两种收敛性进行区分。</p>
<p>读者现在可以理解的是：依概率收敛意味着当 n 充分大时， \xi_n 离 \xi 足够近的概率是很大的。</p>
<p>而几乎处处收敛意味着满足 \lim_{n\rightarrow \infty}\xi_n(\omega)=\xi(\omega) 条件的样本点 \omega 组成的集合的概率测度为1。其实关键问题在于 \lim 与概率测度的位置的不同导致了这两种收敛性的根本区别。</p>
<p>我们现在可能能够模糊地体会到几乎处处收敛比依概率收敛要更强一些。在本章末，我们将会给出证明与反例。届时，我们将能够更加深入地理解这两种收敛性的区别。</p>
<p>我们先来介绍弱大数律。</p>
<p><strong>定理（Chebyshev弱大数律）</strong></p>
<p><strong>X_1,X_2,\dots  为一列两两不相关的随机变量， EX_i=\mu_i 存在且有限， VarX_i=\sigma_i^2 有限</strong></p>
<p><strong>，满足 \exists c\in \mathbb{R},\ \forall i,\ \sigma_i^2\leqslant c （所有方差有共同上界）。则 \frac{S_n-ES_n}{n} \overset{p}{\rightarrow}0(n\rightarrow\infty)</strong></p>
<p>证明：</p>
<p>由于随机变量列两两不相关， VarS<em>n=\sum</em>{i=1}^nVarX_i\leqslant nc</p>
<p>下面验证依概率收敛的正确性：</p>
<p>\forall \epsilon&gt;0,P(|\frac{S_n-ES_n}{n}|\geqslant \epsilon)=P(|S_n-ES_n|\geqslant n\epsilon)</p>
<p>回忆我们之前提到过的Chebyshev不等式，我们有</p>
<p>P(|S_n-ES_n|\geqslant n\epsilon)\leqslant \frac{VarS_n}{n^2\epsilon^2}\leqslant \frac{c}{n\epsilon^2}\rightarrow 0(n\rightarrow\infty)</p>
<p>故 \frac{S_n-ES_n}{n} \overset{p}{\rightarrow}0(n\rightarrow\infty) 。</p>
<p>我们可以看到，<strong>对于特殊情况，当随机变量列中每个随机变量有相同期望 \mu 时，我们可以得到 \frac{S_n}{n}\overset{p}{\rightarrow}\mu(n\rightarrow\infty) 。</strong>这也是经常使用的一种形式。</p>
<p>我们接下来再给出一个弱大数律，它将会回答为何实验次数无限多次时我们可以用频率来逼近概率。</p>
<p><strong>定理（Bernoulli弱大数律）</strong></p>
<p><strong>事件 A 发生的概率为 p 。做独立重复试验，前 n 次试验中 A 发生 v_n 次，则 \frac{v_n}{n}\overset{p}{\rightarrow}p\ (n\rightarrow\infty) 。</strong></p>
<p>证明：</p>
<p>假设 X_i=\begin{cases} 0\ \ 第i次发生\ 1\ \ 第i次不发生 \end{cases}</p>
<p>v<em>n=\sum</em>{i=1}^nX_i ，且 X_1,\dots ,X_n 独立同分布。</p>
<p>由Chebyshev大数律且 EX_i=p ，得到 \frac{v_n}{n}\overset{p}{\rightarrow}p\ (n\rightarrow\infty) 。</p>
<p><strong>Bernoulli弱大数律告诉我们：独立重复试验次数足够多时，频率将会依概率收敛到概率。</strong></p>
<p>事实上，弱大数律仍有许多不同的版本。不同版本的弱大数律或多或少对某一个条件进行了放宽的同时收紧了另一个条件。</p>
<p>我们不加证明地给出：</p>
<p><strong>定理（Khinchine弱大数律）</strong></p>
<p><strong>X_1,X_2,\dots  为一列独立同分布的随机变量， \mu_i=EX_i 存在且有限，则 \frac{S_n}{n}\overset{p}{\rightarrow}\mu(n\rightarrow\infty) 。（注意到不再需要方差有限的条件）</strong></p>
<p>这里要提醒读者：务必记清每个大数律的适用条件。比如：<strong>数学期望有限的条件是不可舍去的。</strong>读者可以自行验证：对于Cauchy分布，其数学期望为 +\infty ，而弱大数律对其不成立。</p>
<p>进行一个总结的话：弱大数律说的是对于满足一定条件的随机变量列，若取出的随机变量个数足够多，这些被取出的随机变量的平均值依概率收敛到其期望。</p>
<p>我们接下来来看强大数律。</p>
<p><strong>定理（Cantelli强大数律）</strong></p>
<p><strong>X_1,X_2,\dots  为一列独立的随机变量， EX_i=\mu_i 存在且有限， \exists M,\ \forall i,\ E(X_i-\mu_i)^4\leqslant M ，则 \frac{S_n-ES_n}{n}\overset{a.s.}{\rightarrow}0\ (n\rightarrow\infty) 。</strong></p>
<p>证明：</p>
<p>这个定理的证明比较冗长。我们先证明一个之后证明的估计中要用到的引理。</p>
<p>引理： X<em>1,X_2,\dots  为一列独立的随机变量， \forall i,\ EX_i=0,EX_i^4\leqslant M ，则 \exists C,s.t.\ E(\sum</em>{i=1}^nX_i)^4\leqslant Cn^2 。</p>
<p>引理证明：</p>
<p>将式子展开，得到 E(\sum<em>{i=1}^nX_i)^4=E(\sum</em>{i=1}^nX<em>i^2)^2+4E(\sum</em>{i&lt;j}X<em>iX_j)^2+4E(\sum_kX_k^2\sum</em>{i&lt;j}X_iX_j)</p>
<p>由于随机变量列相互独立， 4E(\sum<em>kX_k^2\sum</em>{i&lt;j}X_iX_j)=0</p>
<p>故原式 =E\sum<em>{i=1}^nX_i^2+6\sum</em>{i&lt;j}EXi^2X_j^2</p>
<p>由内积不等式， EX_i^2X_j^2\leqslant \sqrt{EX_i^4EX_j^4}\leqslant M</p>
<p>故 \exists C,s.t.\ E(\sum_{i=1}^nX_i)^4\leqslant Cn^2 。</p>
<p>回到强大数律的证明，我们取 \epsilon_n=n^{-\frac{1}{8}}</p>
<p>考虑级数 \sum_{n=1}^\infty P(|\frac{S_n-ES_n}{n}|\geqslant \epsilon_n)</p>
<p>由Markov不等式，原式 \leqslant \sum<em>{n=1}^\infty \frac{E(S_n-ES_n)^4}{n^4\epsilon_n^4}\leqslant \sum</em>{n=1}^\infty\frac{C}{n^2\epsilon_n^4}</p>
<p>\sum<em>{n=1}^\infty\frac{C}{n^2\epsilon_n^4}=C\sum</em>{n=1}^\infty\frac{1}{n^{\frac{3}{2}}}&lt;\infty ，级数收敛。</p>
<p>由Borel-Cantelli引理，我们知道 P(|\frac{S_n-ES_n}{n}|\geqslant \epsilon_n\ \ i.o.)=0</p>
<p>这意味着 P(\lim_{n\rightarrow \infty}\frac{S_n-ES_n}{n}=0)=1</p>
<p>故 \frac{S_n-ES_n}{n}\overset{a.s.}{\rightarrow}0\ (n\rightarrow\infty)</p>
<p>Cantelli强大数律的证明是我所认为的概率论中比较有启发意义的一个证明。它的思想是利用一个趋向于 0 的与 n 有关的 \epsilon_n ，并且证明不可能有无限项大于 \epsilon_n ，逐渐减小上界，以达到证明几乎处处收敛的目的。其中也巧妙地取\epsilon_n=n^{-\frac{1}{8}}，凑出了分母为 n^{\frac{3}{2}} 的收敛级数，过渡到了Borel-Cantelli引理，巧妙地将级数的敛散性与infinitely often的概念连接在了一起。这样连贯的逻辑外加巧妙的设计与取值是值得我们去细细品味的。</p>
<p>Cantelli强大数律的一个推论便是Borel强大数律。</p>
<p><strong>定理（Borel强大数律）</strong></p>
<p><strong>事件 A 发生的概率为 p 。做独立重复试验，前 n 次试验中 A 发生 v_n 次，则 \frac{v_n}{n}\overset{a.s.}{\rightarrow}p\ (n\rightarrow\infty) 。</strong></p>
<p>Borel强大数律给了我们一个关于频率与概率的关系的更强结果：<strong>独立重复试验次数足够多时，频率将会几乎处处收敛到概率。</strong></p>
<p>与o弱大数律类似地，强大数律也有很多不同的形式。</p>
<p>以下再给出几个形式。</p>
<p><strong>定理（Kolmogorov强大数律）</strong></p>
<p><strong>X_1,X_2,\dots  为一列独立同分布随机变量， EX_i=\mu 存在且有限，则 \frac{S_n}{n}\overset{a.s.}{\rightarrow}\mu\ (n\rightarrow\infty) 。（注意到不再需要Cantelli中的四阶中心矩有界的条件）</strong></p>
<p>我们接下来来继续深入讨论一下依概率收敛与几乎处处收敛这两种收敛性之间的关系。</p>
<p><strong>定理（几乎处处收敛强于依概率收敛）</strong></p>
<p><strong>\xi_n\overset{a.s.}{\rightarrow}\xi ，则 \xi_n\overset{p}{\rightarrow}\xi 。</strong></p>
<p>证明：</p>
<p>首先在这里给出几乎处处收敛的一个充要条件。读者可以尝试着自行证明。</p>
<p><strong>引理： \xi<em>n\overset{a.s.}{\rightarrow}\xi 当且仅当 \forall \epsilon&gt;0,P(\bigcap</em>{n=1}^\infty\bigcup_{k=n}^\infty \left { |\xi_k-\xi|\geqslant \epsilon \right })=0 （注意到右侧与 P(|\xi_n-\xi|\geqslant \epsilon\ \ i.o.)=0 等价）</strong></p>
<p>在这里，我们有 \forall \epsilon&gt;0,P(\bigcap<em>{n=1}^\infty\bigcup</em>{k=n}^\infty \left { |\xi_k-\xi|\geqslant \epsilon \right })=0</p>
<p>利用概率的连续性，注意到 \lim<em>{n\rightarrow\infty}P(\bigcup</em>{k=n}^\infty \left { |\xi<em>k-\xi|\geqslant \epsilon \right })\geqslant \lim</em>{n\rightarrow\infty}P(|\xi_n-\xi|\geqslant \epsilon)</p>
<p>所以 \lim_{n\rightarrow\infty}P(|\xi_n-\xi|\geqslant \epsilon)=0 ，即 \xi_n\overset{p}{\rightarrow}\xi 。</p>
<p>为了充分地说明依概率收敛弱于几乎处处收敛，我们在此举一个反例。</p>
<p>例<strong>（依概率收敛但不几乎处处收敛）</strong></p>
<p>X\sim U(0,1) ，考虑样本空间 \Omega=[0,1] ，事件域 F 为 [0,1] 中的Borel可测集全体。对事件 A ，概率测度 P(A)=P(X\in A) 。（事实上，此处取的是Lebesgue测度，即用区间的长度度量概率）</p>
<p>考虑 A_{ij}=<a href="j=1,2,\dots ,i">\frac{j-1}{i},\frac{j}{i}</a></p>
<p>令 B<em>1=A</em>{11},B<em>2=A</em>{21},B<em>3=A</em>{22},B<em>4=A</em>{31},\dots  以此类推</p>
<p>定义随机变量 \xi_n(\omega)=\begin{cases} 1\ \ \ \omega\in B_n\ 0\ \ \ else \end{cases}</p>
<p>则 \forall \epsilon\in (0,1),\ P(\xi_n&gt;\epsilon)=P(\omega\in B_n)\rightarrow 0 （因为 B_n 的长度趋向于0）</p>
<p>\xi_n\overset{p}{\rightarrow}0 成立。</p>
<p>但是， \forall \omega,\xi_n(\omega)\rightarrow0 不成立，故不是几乎处处收敛的。</p>
<p><strong>五、中心极限定理</strong></p>
<p>研究中心极限定理的动机是人们发现：取独立同分布的随机变量，当取的数量很多时，这些随机变量的和的概率分布总是与正态分布的概率密度\p \phi_X(t)=Ee^{itX} 图形很相似。</p>
<p>我们接下来给出中心极限定理及其证明。</p>
<p><strong>定理（Linderberg-Levy中心极限定理）</strong></p>
<p><strong>X_1,X_2,\dots  为一列独立同分布的随机变量，有着相同的数学期望 EX_i=\mu ，相同的方差 VarX_i=\sigma^2 均是有限的。则 \xi_n=\frac{S_n-n\mu}{\sqrt{n\sigma^2}} \overset{d}{\rightarrow}N(0,1)\ (n\rightarrow\infty) 。</strong></p>
<p>证明：</p>
<p>回忆我们讨论特征函数时，讲到过的连续性定理。</p>
<p>这个定理告诉我们 \xi<em>n 依分布收敛到 \xi 当且仅当 \lim</em>{n\rightarrow\infty}\phi<em>{\xi_n}(t)=\phi</em>\xi(t) 。</p>
<p>我们先对 \mu=0,\sigma^2=1 的情形进行证明。</p>
<p>此时 \phi(t)=Ee^{itX_1}</p>
<p>由特征函数的性质， \phi(0)=1,\phi^{‘}(0)=iEX_1,\phi^{‘’}(0)=i^2EX_1^2</p>
<p>容易知道 \phi(0)=1,\phi^{‘}(0)=0,\phi^{‘’}(0)=-1</p>
<p>对于 \phi(t) 进行Taylor展开，得到 \phi(t)=1-\frac{t^2}{2}+o(t^2)\ \ (t\rightarrow 0)</p>
<p>此时 \xi<em>n=\frac{\sum</em>{i=1}^nX<em>i}{\sqrt{n}} ，故 \phi</em>\xi(t)=Ee^{it\xi<em>n}=\prod</em>{i=1}^nEe^{i\frac{t}{\sqrt{n}}X_i}=\phi^n(\frac{t}{\sqrt{n}})</p>
<p>\phi_\xi(t)=[1-\frac{t^2}{2n}+o(t^2)]^n</p>
<p>故 \lim<em>{n\rightarrow\infty}\phi</em>\xi(t)=\lim_{n\rightarrow\infty}[1-\frac{t^2}{2n}+o(t^2)]^n=e^{-\frac{t^2}{2}}</p>
<p>由连续性定理， \xi_n=\frac{S_n}{\sqrt{n}}\overset{d}{\rightarrow}N(0,1) ，得证。</p>
<p>对于 \mu,\sigma^2 随意取值的一般情况，考虑其标准化 Y_i=\frac{X_i-\mu}{\sigma} ，很容易证明结论的正确性。</p>
<p>中心极限定理具有极其重要的意义。它告诉我们：只要有足够多的独立同分布的随机变量，我知道了数学期望与方差，我便已经能够求出他们之和的概率分布。今后在遇到大样本（ 一般认为n\geqslant 30 ）的独立同分布随机变量，我们如果要求和的分布，它们各自的分布已经无足轻重。两个数字特征（期望与方差）就能够决定和的分布。</p>
<p>事实上，独立但是不同分布的随机变量列也具有中心极限定理。</p>
<p>下面我们总是假设 B<em>n^2=\sum</em>{i=1}^nVarX_i （为列中所有随机变量方差之和）</p>
<p>我们不加证明地给出如下中心极限定理。</p>
<p><strong>定理（Linderberg-Feller中心极限定理）</strong></p>
<p><strong>X<em>1,X_2,\dots  为一列独立的随机变量且满足 \lim</em>{n\rightarrow\infty}B<em>n=\infty,\lim</em>{n\rightarrow\infty}\frac{VarX<em>n}{B_n^2}=0 （随机变量列中每一个随机变量都没有决定性作用，方差趋向于 0与 \infty 都不太快），则 \frac{S_n-ES_n}{B_n} \overset{d}{\rightarrow}N(0,1)\ (n\rightarrow\infty) 的充要条件是Linderberg条件成立。（ \lim</em>{n\rightarrow\infty}\frac{1}{B<em>n^2}\sum</em>{i=1}^nE((X_i-\mu_i)^2I[|X_i-\mu_i|\geqslant \epsilon B_n])=0 ）</strong></p>
<p>作为推论，我们还能够得到Liapunoff中心极限定理。</p>
<p><strong>定理（Liapunoff中心极限定理）</strong></p>
<p><strong>X<em>1,X_2,\dots  为一列独立随机变量，若存在常数 \delta&gt;0 ，使得\lim</em>{n\rightarrow\infty}\frac{1}{B<em>n^{2+\delta}}\sum</em>{i=1}^nE|X_i-\mu_i|^{2+\delta}=0 ，则 \frac{S_n-ES_n}{B_n} \overset{d}{\rightarrow}N(0,1)\ (n\rightarrow\infty)</strong></p>
<p>证明：</p>
<p>事实上，只需要验证Linderberg条件成立即可。</p>
<p>\lim<em>{n\rightarrow\infty}\frac{1}{B_n^2}\sum</em>{i=1}^nE((X<em>i-\mu_i)^2I[|X_i-\mu_i|\geqslant \epsilon B_n])\leqslant \lim</em>{n\rightarrow\infty}\frac{1}{B<em>n^2}\sum</em>{i=1}^nE(\frac{(X_i-\mu_i)^{2+\delta}}{(\epsilon B_n)^\delta}I[|X_i-\mu_i|\geqslant \epsilon B_n])</p>
<p>\leqslant\lim<em>{n\rightarrow\infty}\frac{1}{B_n^{2+\delta}\epsilon^\delta}\sum</em>{i=1}^nE|X_i-\mu_i|^{2+\delta}=0</p>
<p>故得证。</p>
<p>最后，我们再讨论一下依分布收敛与依概率收敛、几乎处处收敛的关系。</p>
<p><strong>定理（依概率收敛强于依分布收敛）</strong></p>
<p><strong>若 \xi_n\overset{p}{\rightarrow}\xi ，则 \xi_n\overset{d}{\rightarrow}\xi 。</strong></p>
<p>证明：</p>
<p>\xi_n 分布函数为 F_n ， \xi 分布函数为 F 。</p>
<p>对任意 F 的连续点 x ，取 \delta&gt;0 。</p>
<p>F_n(x)-F(x)=P(\xi_n\leqslant x)-F(x)=P(\xi_n\leqslant x,\xi&gt;x+\delta)+P(\xi_n\leqslant x,\xi\leqslant x+\delta)-F(x)</p>
<p>\leqslant P(|\xi_n-\xi|&gt;\delta)+F(x+\delta)-F(x)</p>
<p>同理地， F(x)-F_n(x)=P(\xi_n&gt;x)-P(\xi&gt;x)=P(\xi_n&gt;x,\xi\leqslant x-\delta)+P(\xi_n\leqslant x,\xi&gt;x-\delta)-P(\xi&gt;x)</p>
<p>\leqslant P(|\xi_n-\xi|&gt;\delta)+F(x)-F(x-\delta)</p>
<p>则 n\rightarrow\infty 时， \overline{\lim_{n\rightarrow\infty}}|F_n(x)-F(x)|\leqslant F(x+\delta)-F(x-\delta)</p>
<p>令 \delta\rightarrow 0 ，由 x 处 F 的连续性，得到 \overline{\lim_{n\rightarrow\infty}}|F_n(x)-F(x)|=0</p>
<p>由上极限的性质， \lim_{n\rightarrow\infty}F_n(x)=F(x) ，得证。</p>
<p>为了更充分地说明这一点，我们举一个反例。</p>
<p>例<strong>（依分布收敛但是不依概率收敛）</strong></p>
<p>\xi_n 为一列独立同分布的随机变量，都服从标准正态分布， \xi\sim N(0,1) 。</p>
<p>则显然 \xi_n\overset{d}{\rightarrow}\xi 。</p>
<p>但是 \xi_n-\xi\sim N(0,2)</p>
<p>故 P(|\xi_n-\xi|\geqslant 1)&gt;0 ，所以不依概率收敛。</p>
<h1 id="概率论学习笔记（六）"><a href="#概率论学习笔记（六）" class="headerlink" title="概率论学习笔记（六）"></a>概率论学习笔记（六）</h1><p><a target="_blank" rel="noopener" href="https://www.zhihu.com/people/zhou-hao-cheng-30"><img src="https://pic1.zhimg.com/v2-53d7fc04a71ec457c22be74dc42c7b09_l.jpg?source=172ae18b" alt="sola"></a></p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/people/zhou-hao-cheng-30">sola</a><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/48509984">​</a></p>
<p>数学话题下的优秀答主</p>
<p>​关注他</p>
<p>作为概率论笔记的收尾，我准备在最后介绍一些概率论偏向于实际的应用。具体地来说，先对需要用到的概率论知识进行些许补充，然后介绍一些风险管理的基础知识。</p>
<p>若之后仍有空余时间，我将补充一篇额外的笔记包含一些经典习题以及其解题思路给读者作为参考。</p>
<p>我们先来介绍一些补充知识。这其中有些是后文将会用到的，另一些则是对概率论相关内容的扩展。</p>
<p><strong>一、分位数，众数，矩母函数，数学期望的其它性质</strong></p>
<p>关于数据特征，我们此处额外介绍一下分位数、众数。</p>
<p>X 为随机变量。</p>
<p><strong>定义：</strong></p>
<p><strong>0&lt;p&lt;1, 若存在 c 使得 P(X\leqslant c)\geqslant p,P(X\geqslant c)\geqslant 1-p ，则称 c 为 X 的 p- 分位数。</strong></p>
<p><strong>若 X 在点 c 处取值的概率最大，称 c 为 X 的众数。</strong></p>
<p>特别地， 0.5- 分位数就是中位数。</p>
<p>对于众数 c 而言，若 X 为离散型随机变量， P(X=c) 应该是概率分布列中的最大值。而若 X 为连续型随机变量，容易知道在 c 处密度 f(x) 取最大值。</p>
<p>需要注意的是：分位数与众数并不是唯一的。下举一例说明。</p>
<p>例：</p>
<p>P(X=1)=P(X=2)=\frac{1}{2} 。</p>
<p>考虑中位数，我们发现按照如上定义，任何 [1,2] 中实数均为中位数。而 1 与 2 均为众数。</p>
<p>接下来，我们介绍与特征函数具有相似定义与性质的矩母函数。</p>
<p><strong>定义： M_X(t)=Ee^{tX} 称为随机变量 X 的矩母函数。</strong></p>
<p>关于其性质，我们简单地介绍两条。（事实上，所有性质都可由特征函数推广而来）</p>
<p><strong>1、 M_X(0)=1,M_X^{(n)}(0)=EX^n</strong></p>
<p><strong>2、 X<em>1,\dots ,X_n 为相互独立的随机变量， X_i 矩母函数为 M_i(t) ，则对于 Y=\sum</em>{i=1}^nX<em>i ，其有矩母函数 M_Y(t)=\prod</em>{i=1}^nM_i(t) 。</strong></p>
<p>容易看到矩母函数同样很适合处理独立的随机变量之和的分布的问题。</p>
<p>与特征函数类似地，<strong>矩母函数也与概率分布相互唯一决定</strong>。且矩母函数在 0 处的 n 阶导数值即为 X 的 n 阶原点矩，这就是“矩母函数”这个名称的由来。为理解这些性质，在此举一例进行说明。</p>
<p>例：已知矩母函数 M_X(t)=e^{2e^t-2} ，求 EX,VarX,P(X=2)</p>
<p>解：</p>
<p>EX=M<em>X^{‘}(t)|</em>{t=0}=2</p>
<p>EX^2=M<em>X^{‘’}(t)|</em>{t=0}=6</p>
<p>故 VarX=EX^2-E^2X=2</p>
<p>此处要如何求得概率分布呢？</p>
<p>注意到 M_X(t)=Ee^{tX} ，要是我们能将矩母函数写成 P(X=0)e^{t0}+P(X=1)e^{t}+P(X=2)e^{2t}+\dots  的形式，问题就迎刃而解了。</p>
<p>仔细观察矩母函数的形式，我们发现进行Taylor展开可以将指数上的指数函数拿下来，编变为上述形式。</p>
<p>故我们在 0 处进行Taylor展开。</p>
<p>得到 M_X(t)=e^{-2}e^{2e^t}=e^{-2}(1+2e^t+\frac{(2e^t)^2}{2!}+\dots )</p>
<p>则易知 P(X=2)=\frac{2}{e^2} 。</p>
<p>接着，我们再介绍一下数学期望的其他性质。</p>
<p>我们知道在定义数学期望时，我们将离散型随机变量和连续型随机变量进行了分开定义，而且我们还遗漏了对于混合型随机变量的数学期望定义。（是我故意的233）那么，是否存在一种统一定义对于这三种随机变量具有相同的形式呢？我们给出数学期望的另一种定义。</p>
<p><strong>性质：（数学期望等价定义）</strong></p>
<p><strong>假设 X\in [a,\infty)\ a.s. ， F 为分布函数，那么 EX=a+\int _a^\infty(1-F(x))\mathrm dx  。</strong></p>
<p>证明：</p>
<p>我们在此仅对于连续型随机变量进行证明，离散型与混合型的证明由读者完成。</p>
<p>a+\int _a^\infty(1-F(x))\mathrm dx =a+\int_a^\infty P(X&gt;x)\mathrm dx =a+\int_a^\infty \mathrm dx \int_x^\infty f(t)dt</p>
<p>由Fubini定理，可以进行二重积分变序，得到 =a+\int_a^\infty f(t)dt\int_a^t\mathrm dx </p>
<p>=a+\int_a^\infty t f(t)dt-a\int_a^\infty f(t)dt=\int_a^\infty tf(t)dt=EX</p>
<p>得证。</p>
<p>值得一提的是，这个形式可以进行推广。当 X\in[a,b]\ a.s. 时， EX=a+\int_a^b(1-F(x))\mathrm dx  。证明由读者自行完成。</p>
<p>这个等价定义利用了分布函数，好处是<strong>对离散型、连续型、混合型随机变量的数学期望求解具有统一的定义形式。</strong></p>
<p>对于，数学期望的性质，我们再补充一下Jensen不等式的数学期望形式。这在证明一些与期望相关的关系式时往往很有用。</p>
<p><strong>定理（Jensen不等式的数学期望形式）</strong></p>
<p><strong>h 为实函数，二阶可导。则若 \frac{d^2}{\mathrm dx ^2}h(x)\geqslant 0 对任意 X 的取值概率/密度非零点成立，则 Eh(X)\geqslant h(EX) 。反之，若 \frac{d^2}{\mathrm dx ^2}h(x)\leqslant 0 对任意 X 的取值概率/密度非零点成立，则 Eh(X)\leqslant h(EX)。</strong></p>
<p>略去此定理的证明。</p>
<p>我们知道在分析学课程中我们接触到的Jensen不等式比较的是 f(\frac{x_1+\dots +x_n}{n}) 与 \frac{f(x_1)+\dots +f(x_n)}{n} 的大小关系。这其实仅仅是上述定理的特殊情况。（取概率分布为 P(X=x_i)=\frac{1}{n}(i=1,2,\dots ,n) 即可）</p>
<p>事实上，期望本身也带有加权平均的意义，因此Jensen不等式在期望意义下也可以进行推广。</p>
<p>我们如果<strong>取 h(x)=x^2 ，则可以得到 EX^2\geqslant E^2X ，事实上，这与方差 VarX=EX^2-E^2X\geqslant 0 是统一的。</strong></p>
<p><strong>取 h(x)=\sqrt{x} ，则对于非负随机变量 X ，得到 E\sqrt{X}&lt;\sqrt{EX} 。（此处二阶导数不为 0 ，为严格上凸函数，可以改为严格不等号），这也是一个很有用的关系式。</strong></p>
<p><strong>二、次序统计量，混合分布</strong></p>
<p>次序统计量这个概念的产生源于人们想求得若干个独立的随机变量在排序之后所具有的概率分布。</p>
<p><strong>定义：</strong></p>
<p><strong>X<em>1,\dots ,X_n 为随机变量，对 \omega\in\Omega ，将 X_1(\omega),\dots ,X_n(\omega) 从小到大进行排序，得到的 X</em>{(1)}(\omega),\dots ,X_{(n)}(\omega) 称为原随机变量的次序统计量。</strong></p>
<p>特别地，在实际应用中我们最常碰到的情况是连续型的独立同分布随机变量的次序统计量。</p>
<p>为了简化，我们仅仅研究<strong>当 X_1,\dots ,X_n 为独立同分布的连续型随机变量</strong>时的情况。</p>
<p>以下假设出现的随机变量都是独立同分布的连续型随机变量，所有 X_i 有公共的密度 f(x) 与分布 F(x) 。</p>
<p>我们容易证明： (X<em>{(1)},\dots ,X</em>{(n)}) 在这样的条件下为<strong>连续型随机向量</strong>。（读者自证）其联合分布假设为 F_n(x_1,\dots ,x_n) ，则联合分布函数连续。</p>
<p>我们现在想要求得每一个次序统计量所具有的分布，不妨从联合密度入手。先求出联合密度，再求边缘密度。我们下面开始推导其联合密度。</p>
<p>由于(X<em>{(1)},\dots ,X</em>{(n)}) 为连续型随机向量，可得 P(X<em>{(1)}&lt;X</em>{(2)}&lt;\dots &lt;X_{(n)})=1 。</p>
<p>对于区域 D=\left {(x_1,\dots ,x_n)|a_i&lt;x_i\leqslant b_i,i=1,2,\dots ,n \right }</p>
<p>考虑概率 P((X<em>{(1)},\dots ,X</em>{(n)})\in D)=P((X<em>{(1)},\dots ,X</em>{(n)})\in D,X<em>{(1)}&lt;X</em>{(2)}&lt;\dots &lt;X_{(n)})</p>
<p>假设 i_1,i_2,\dots ,i_n 为 1,2,\dots ,n 的一个全排列，则</p>
<p>原式 =\sum<em>{i_1,\dots ,i_n} P((X</em>{i<em>1},\dots ,X</em>{i<em>n})\in D,X</em>{i<em>1}&lt;X</em>{i<em>2}&lt;\dots &lt;X</em>{i_n})</p>
<p>=n!P((X<em>{1},\dots ,X</em>{n})\in D,X<em>{1}&lt;X</em>{2}&lt;\dots &lt;X_{n})</p>
<p>=\int\dots \int_Dn!f(x_1)f(x_2)\dots f(x_n)I[x_1&lt;x_2&lt;\dots &lt;x_n]\mathrm dx _1\dots \mathrm dx _n</p>
<p>由联合密度的定义，我们得到了一个重要的结果：</p>
<p><strong>随机向量 (X<em>{(1)},\dots ,X</em>{(n)}) 具有联合密度 f_n(x_1,\dots ,x_n)=\begin{cases} n!f(x_1)\dots f(x_n)\ \ \ \ x_1&lt;x_2&lt;\dots &lt;x_n\ 0\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ else \end{cases}</strong></p>
<p>那么对于我们所关注的 X_{(k)} 的概率分布，我们只需要求边缘密度即可。设其密度为 f_k(x_k) 。</p>
<p>那么 f<em>k(x_k)=n!f(x_k)\int\dots \int</em>{-\infty&lt;x<em>1&lt;\dots &lt;x_k}f(x_1)\dots f(x</em>{k-1})\mathrm dx <em>1\dots \mathrm dx </em>{k-1} \times \int\dots \int<em>{x_k&lt;x</em>{k+1}&lt;\dots &lt;x<em>n&lt;+\infty}f(x</em>{k+1})\dots f(x<em>n)\mathrm dx </em>{k+1}\dots \mathrm dx _n</p>
<p>那么问题转化为了如何求这个积分。</p>
<p>在此我们给出如下<strong>引理：</strong></p>
<p><strong>对于 -\infty\leqslant a&lt;x<em>1&lt;x_2&lt;\dots &lt;x_k&lt;b\leqslant +\infty ， F为分布， f为密度，则 \int\dots \int</em>{a&lt;x_1&lt;\dots &lt;x_k&lt;b}f(x_1)\dots f(x_k)\mathrm dx _1\dots \mathrm dx _k=\frac{(F(b)-F(a))^k}{k!}。</strong></p>
<p>这个引理交给读者利用数学归纳法与Fubini定理完成证明。</p>
<p>利用这个引理，我们便可以求出上述积分，<strong>得到X_{(k)} 的密度</strong></p>
<p><strong>f(x_k)=n!f(x_k)\frac{F^{k-1}(x_k)}{(k-1)!}\frac{[1-F(x_k)]^{n-k}}{(n-k)!}</strong></p>
<p>至此，我们已经求出了<strong>每一个次序统计量的概率分布</strong>。</p>
<p>我们举例来进一步说明。</p>
<p>例：X_1,\dots ,X_4\sim\varepsilon(\lambda) ，独立同分布，求 min(X_1,\dots ,X_4) 与 max(X_1,\dots ,X_4) 的密度。</p>
<p>解：</p>
<p>X<em>{(1)}=min(X_1,\dots ,X_4) ， X</em>{(4)}=max(X_1,\dots ,X_4)</p>
<p>利用我们上面得到的关于次序统计量的密度的结论。</p>
<p>其中 f(x)=\lambda e^{-\lambda x},F(x)=1-e^{-\lambda x}</p>
<p>得到 f<em>1(x)=4\lambda e^{-4\lambda x} 为 X</em>{(1)} 的密度。</p>
<p>f<em>4(x)=4\lambda e^{-\lambda x}(1-e^{-\lambda x})^3 为 X</em>{(4)} 的密度。</p>
<p>我们发现： min(X_1,\dots ,X_4)\sim\varepsilon(4\lambda) 。这告诉我们：若干个独立同分布的指数分布随机变量，其最小值仍服从指数分布。</p>
<p>这里值得一提的一点是：应用得最多的往往是 X<em>{(1)},X</em>{(n)} <strong>（最大值与最小值）</strong>这两个次序统计量的分布。然而，这两者的分布却有着更加容易求出的方法：<strong>分布函数法</strong>。</p>
<p>此处利用分布函数法重新计算上例中 X_{(1)} 的密度作为示范：</p>
<p>假设其分布函数为 F_1 ，密度为 f_1</p>
<p>F_1(x)=P(min(X_1,\dots ,X_4)\leqslant x)=1-P(min(X_1,\dots ,X_4)&gt;x)</p>
<p>=1-P(X_1&gt;x)\dots P(X_4&gt;x)=1-P^4(X_1&gt;x)</p>
<p>=1-(e^{-\lambda x})^4</p>
<p>所以 f_1=F_1^{‘}=4\lambda e^{-4\lambda x} 。</p>
<p>这样的方法是更加简便容易计算的。X_{(4)} 的密度的计算作为练习留给读者。</p>
<p>接下来，我们再来介绍一下混合分布的概念。</p>
<p>混合分布是一个很有趣的概念。它被定义的出发点是为了解决这样一个问题：已知 X_1,X_2 这两个随机变量的概率分布，现在有随机变量 Y ， Y 有概率 p 具有和 X_1 同样的分布，有概率 1-p 具有和 X_2 同样的分布，那么 Y 的分布如何求出？</p>
<p>在接下来的讨论中，我们<strong>假设 X_1,X_2 均为连续型随机变量，密度为 f_1,f_2 ，Y 有概率 p 具有和 X_1 同样的分布，有概率 1-p 具有和 X_2 同样的分布。</strong></p>
<p>读者容易证明 <strong>Y 必定为连续型随机变量，设其密度为 f</strong> 。</p>
<p>我们现在来求出密度 f 。</p>
<p>由密度的定义，我们考虑对于区间 (a,b] ，</p>
<p>考虑概率 P(Y\in(a,b]) ，利用全概公式，得到其 =pP(X_1\in(a,b])+(1-p)P(X_2\in(a,b])</p>
<p>=p\int_a^bf_1(x)\mathrm dx +(1-p)\int_a^bf_2(x)\mathrm dx =\int_a^b[pf_1(x)+(1-p)f_2(x)]\mathrm dx </p>
<p><strong>由密度定义，密度 f(y)=pf_1(y)+(1-p)f_2(y)</strong></p>
<p>我们发现对于这样的随机变量，其密度为 X_1,X_2 的密度的加权平均，权重为 p,1-p 。</p>
<p><strong>我们称这样的随机变量 Y 为 X_1,X_2 以权重 p,1-p 生成的混合分布随机变量。</strong></p>
<p>需要注意的是：同样含有“混合”二字，具有混合分布的随机变量与混合型随机变量的含义截然不同。（不清楚的读者请翻阅前几篇笔记）</p>
<p>对于混合分布随机变量，有很多有趣而常用的性质：</p>
<p><strong>1、 EY^k=pEX_1^k+(1-p)EX_2^k</strong></p>
<p>证明：</p>
<p>EY^k=\int<em>{-\infty}^{+\infty} y^kf(y)\mathrm dy=p\int</em>{-\infty}^{+\infty}y^kf<em>1(y)\mathrm dy+(1-p)\int</em>{-\infty}^{+\infty}y^kf_2(y)\mathrm dy</p>
<p>=pEX_1^k+(1-p)EX_2^k</p>
<p><strong>2、 P(Y\leqslant y)=pP(X_1\leqslant y)+(1-p)P(X_2\leqslant y)</strong></p>
<p>证明：</p>
<p>F(y)=\int<em>{-\infty}^yf(t)dt=p\int</em>{-\infty}^yf<em>1(t)dt+(1-p)\int</em>{-\infty}^yf_2(t)dt</p>
<p>=pF_1(y)+(1-p)F_2(y)</p>
<p><strong>3、 M_Y(t)=pM_1(t)+(1-p)M_2(t) ，其中 M_i(t) 为 X_i 的矩母函数。</strong></p>
<p>（矩母函数本质也是数学期望，同理得证）</p>
<p>我们看到：<strong>混合分布随机变量关于两个生成的随机变量在概率、分布函数、任意阶原点矩上均为线性组合，这为计算提供了很大的便利。这些性质产生的本质原因就是因为其密度是线性组合。</strong></p>
<p>需要注意的是：<strong>这并不意味着 Y=pX_1+(1-p)X_2 。</strong>牢记混合分部随机变量指的是密度的线性组合，而非随机变量的线性组合。</p>
<p>这也<strong>不意味着混合分布随机变量的方差是生成随机变量方差的线性组合</strong>。（回忆方差为二阶中心矩，读者很容易找到反例）</p>
<p>一种常常应用的特殊情形是 X_1 恒为 0 的情况。</p>
<p>此时 Y 有概率 p 为 0 ，有概率 1-p 与 X_2 同分布。</p>
<p>特别地，这种情形下我们有 EY^k=(1-p)EX_2^k 等性质，此处不再赘述。</p>
<p><strong>三、风险管理</strong></p>
<p>在这一章中，我们介绍一些风险管理中的基本概念与知识，作为概率论在实际生活中的应用。（虽然实际的模型会远比这些复杂）</p>
<p>我们向大家介绍一些<strong>简单的保险模型</strong>。</p>
<p>我们都知道保险的工作机制是：购买保险的人需事先交保费。若损失发生，则保险将会进行赔付。</p>
<p>接下来，我们<strong>总是用 X 作为表示损失数额的随机变量，用</strong> Y <strong>作为表示赔付数额的随机变量，总是假设</strong> X,Y <strong>均为连续型随机变量， X 的密度为 f(x) 。</strong>若无特别说明，我们总是假设保险会赔付全部损失额 X ，那么容易知道纯保费（保险费中用于支付保险赔偿金的部分）与期望的赔付都应该为 EX 。</p>
<p>我们接下来主要想研究的则是部分保险（赔付损失额不完全等于损失额 X 的保险）</p>
<p><strong>1、免赔额保险（de\mathrm ductible insurance）</strong></p>
<p><strong>免赔额保险均设有一个免赔额 d 。当损失发生时，如果损失低于免赔额，不进行赔付；如果损失高于免赔额，赔付 X-d 。</strong></p>
<p><strong>即 Y=\begin{cases} 0\ \ \ \ \ \ \ \ \ \ \ X\leqslant d\ X-d\ \ \ X&gt;d \end{cases} =max(X-d,0)</strong></p>
<p>我们关心的是：期望赔付额 EY 将会是多少。</p>
<p>首先，容易发现 Y 是我们前面讲到过的<strong>混合分布随机变量</strong>。</p>
<p>其有 P(X\leqslant d) 的概率为 0 ，有 P(X&gt;d) 的概率与 X-d 同分布。</p>
<p>由混合分布随机变量对于期望线性可拆分的性质，（也可以利用全期望公式）</p>
<p>我们有 EY=P(X&gt;d)E(X-d|X&gt;d)</p>
<p>回忆我们在条件数学期望中介绍的定理：</p>
<p><strong>A为事件，P(A)&gt;0,E(X|A)存在，则E(X|A)=\frac{E(XI[A])}{P(A)}。</strong></p>
<p>我们得到 E(X-d|X&gt;d)=\frac{E((X-d)I[X&gt;d])}{P(X&gt;d)}</p>
<p><strong>故 EY=E((X-d)I[X&gt;d])=\int_d^{+\infty}(x-d)f(x)\mathrm dx  。</strong></p>
<p>利用期望的等价定义（分布函数），我们也可以证明 EY=\int_d^{+\infty}[1-F_X(x)]\mathrm dx  。</p>
<p>在此处，我们再补充几种其它的免赔额类型的保险，但是不作详细展开，读者可以自行验证其期望赔付额作为练习。</p>
<p><strong>（1）：起赔式免赔额保险（Franchise de\mathrm ductible）</strong></p>
<p><strong>免赔额为 d 。当损失额小于免赔额，不赔付。当损失额超过免赔额，赔付全部损失。</strong></p>
<p><strong>即 Y=\begin{cases} 0\ \ \ \ X\leqslant d\ X\ \ \ X&gt;d \end{cases} ， EY=\int_d^{+\infty}xf(x)\mathrm dx  。</strong></p>
<p><strong>（2）：隐藏式免赔额保险（Disappearing de\mathrm ductible）</strong></p>
<p><strong>设有免赔额下限 d ，免赔额上限 d’ （ d&lt;d’ ）。若损失额小于免赔额下限，不赔付。若损失额大于免赔额上限，赔付全部损失。若损失额处于二者之间，赔付 d’\frac{X-d}{d’-d} （线性调整）。</strong></p>
<p><strong>即 Y=\begin{cases} 0\ \ \ \ \ \ \ \ \ \ \ X\leqslant d\ d’\frac{X-d}{d’-d}\ \ \ d&lt;X\leqslant d’\ X\ \ \ \ \ \ \ \ \ \ X&gt;d’ \end{cases} ， EY=\int<em>d^{d’}d’\frac{x-d}{d’-d}f(x)\mathrm dx +\int</em>{d’}^{+\infty}xf(x)\mathrm dx  。</strong></p>
<p>我们不难发现：一般的免赔额保险与隐藏式免赔额保险的赔付额随机变量 Y 为关于损失额 X 的连续函数。</p>
<p>我们试举一例帮助读者理解。</p>
<p>例：</p>
<p>损失额随机变量 X 服从指数分布，且其数学期望为 1000 ，保险免赔额为 200 ，求损失发生时的期望赔付额。</p>
<p>解：</p>
<p>由于指数分布 \varepsilon(\lambda) 的数学期望为 \frac{1}{\lambda} ，故 X\sim \varepsilon(\frac{1}{1000}) 。</p>
<p>则 EY=\int_{200}^{+\infty}(x-200)\frac{e^{-\frac{x}{1000}}}{1000}\mathrm dx </p>
<p>=-xe^{-\frac{x}{1000}}-1000e^{-\frac{x}{1000}}+200e^{-\frac{x}{1000}}|_{x=200}^\infty=1000e^{-\frac{1}{5}}</p>
<p><strong>2、带有保险限额的保险（Policy limit）</strong></p>
<p><strong>此类保险设有保险限额 u 。若损失额小于保险限额，赔付所有损失。若损失额大于保险限额，赔付额等于保险限额。</strong></p>
<p><strong>即 Y=\begin{cases} X\ \ \ \ X\leqslant u\ u\ \ \ \ \ X&gt;u \end{cases} 。</strong></p>
<p><strong>同样地，利用混合分布随机变量的性质以及条件数学期望的性质，容易算出 EY=\int_0^uxf(x)\mathrm dx +u\int_u^{+\infty}f(x)\mathrm dx </strong></p>
<p><strong>利用数学期望的分布函数等价定义来表示，得到 EY=\int_0^u[1-F_X(x)]\mathrm dx </strong></p>
<p>计算和证明都交给读者自己完成。</p>
<p>更复杂一些地，我们可以将免赔额与保险限额相结合起来。</p>
<p><strong>3、同时带有免赔额和保险限额的保险</strong></p>
<p><strong>设有免赔额 d ，保险限额 u ，那么赔付额 Y=\begin{cases} 0\ \ \ \ \ \ \ \ \ \ \ X\leqslant d\ X-d \ \ \ d<X\leqslant u\\ u-d \ x>u \end{cases} 。</X\leqslant></strong></p>
<p><strong>这里注意需要先考虑保险限额的限制，再加上免赔额的限制。</strong></p>
<p><strong>容易得到赔付额的期望 EY=\int_d^u(x-d)f(x)\mathrm dx +(u-d)\int_u^{+\infty}f(x)\mathrm dx </strong></p>
<p><strong>利用等价定义，可以得到 EY=\int_d^u[1-F_X(x)]\mathrm dx  。</strong></p>
<p>以上便就是几种简单的保险模型。</p>
<p>最后，我们介绍一下保险政策的总理赔额模型。</p>
<p>我们先引入<strong>个人风险模型</strong>。</p>
<p>我们<strong>假设共有 n 个人购买了保险，其赔付额分别为 P_1,P_2,\dots ,P_n 。以下总是假定每个人的赔付额相互独立。</strong></p>
<p>我们<strong>用 S 来表示保险公司总的赔付额，则 S=\sum_{i=1}^nP_i 。</strong></p>
<p>由于相互独立性，我们同时有 ES=\sum<em>{i=1}^nEP_i,VarS=\sum</em>{i=1}^nVarP_i 。</p>
<p>当购买保险的人数足够多时 (n\rightarrow\infty) ，我们就可以<strong>利用Linderberg-Levy中心极限定理对于 S 的分布进行正态近似（实际应用数量大于</strong> 30 <strong>即可）。</strong>（回忆：当中心极限定理适用条件成立时，我们只需要知道 ES 与 VarS 便可以确定 S 的概率分布）</p>
<p>通常，我们这里<strong>取 S 的 0.95- 分位数 Q 。</strong>（意味着 Q 有 0.95 的概率比 S 更大）</p>
<p>如果<strong>保险公式收集到的保费不少于 Q ，那么保险公司就有 0.95 的概率会盈利。</strong></p>
<p>实际中的情况是：<strong>P_i 往往不是同分布的。（保险公司在投保人投保之前往往会对其风险进行评估，让不同风险等级的人购买不同的保险）但是，处理方法是类似的，我们仍然可以利用Linderberg-Feller中心极限定理</strong>。我们用例子来说明这样的处理方法。</p>
<p><strong>例：</strong></p>
<p><strong>保险公司有三种独立的保险政策（每个人仅能同时投保其中一种）。以下是这些政策与顾客的投保情况：</strong></p>
<p><strong>政策1： 500 人投保，每个个体发生损失的概率为 0.05 ，若个体的损失发生，则赔付额的期望为 5 ，方差为 5 。</strong></p>
<p><strong>政策2： 1000 人投保，每个个体发生损失的概率为 0.1 ，若个体的损失发生，则赔付额的期望为 10 ，方差为 10 。</strong></p>
<p><strong>政策3： 500 人投保，每个个体发生损失的概率为 0.15 ，若个体的损失发生，则赔付额的期望为 5 ，方差为 5 。</strong></p>
<p><strong>现在已知收取的总保费与总理赔额的期望成正比例，比例系数为 \theta 。求 \theta 最小为多少时可以保证保险公司亏本的概率不高于 0.05 。</strong></p>
<p>解：</p>
<p>共有 2000 人投保，设投保政策1的人的每个人的赔付额分别为 P<em>1,\dots ,P</em>{500} ，政策2为 P<em>{501},\dots ,P</em>{1500} ，政策3为 P<em>{1501},\dots ,P</em>{2000} 。</p>
<p>设总保费为 Q ，则 Q=\theta S 。</p>
<p>现在要保证 P(Q\geqslant S)=0.95 。</p>
<p>我们要求出 S 的概率分布，自然要利用中心极限定理。这意味着，我们只需要求出 ES,VarS 即可确定其分布。</p>
<p>ES=500EP<em>1+1000EP</em>{1500}+500EP_{2000}</p>
<p>VarS=500VarP<em>1+1000VarP</em>{1500}+500VarP_{2000}</p>
<p>但是， P_i 本质上是一个混合分布的随机变量。</p>
<p>P_1 有 0.95 的概率为 0 ，有 0.05 的概率与 B_1 同分布。而我们知道的是： EB_1=5,VarB_1=5 。</p>
<p>回忆混合分布随机变量的期望的线性可分性。</p>
<p>我们得到： EP_1=0.05EB_1,EP_1^2=0.05EB_1^2 （计算二阶原点矩是为了计算方差，注意到方差并不具有线性可分性）</p>
<p>而 EB_1^2=VarB_1+E^2B_1=30</p>
<p>故我们求得 EP_1=0.25,EP_1^2=1.5</p>
<p>故 VarP_1=EP_1^2-E^2P_1=1.4375</p>
<p>对于 P<em>{1500},P</em>{2000} ，我们进行同样的操作。可以算出 EP<em>{1500}=1,VarP</em>{1500}=10 ， EP<em>{2000}=0.75,VarP</em>{2000}=3.9375 。</p>
<p>所以我们得到 ES=1500,VarS=12687.5 。</p>
<p>这告诉我们： S\overset{d}{\rightarrow}N(1500,12687.5)</p>
<p>回忆利用正态分布求概率的技巧（化为标准正态分布求解）</p>
<p>我们得到： \Phi(\frac{Q-1500}{\sqrt{12687.5}})=0.95</p>
<p>解得 Q=1685.29</p>
<p>故 \theta=1.1235 为其最小值。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://kobicgend.top">小明同學</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章連結: </span><span class="post-copyright-info"><a href="http://kobicgend.top/posts/0.html">http://kobicgend.top/posts/0.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版權聲明: </span><span class="post-copyright-info">本站所有文章除特別聲明外，均採用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 許可協議。轉載請註明來自 <a href="http://kobicgend.top" target="_blank">小明の雜貨鋪</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="/img/avatar.png" data-sites="wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/0.html" title=""><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info"></div></div></a></div><div class="next-post pull-right"><a href="/posts/f462e7a0.html" title="掷地有声｜一封来自童年的回信"><img class="cover" src="/img/posts/arts_cucradio/arts_cucradio20240529_head.webp" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">掷地有声｜一封来自童年的回信</div></div></a></div></nav><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 評論</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">小明同學</div><div class="author-info__description">「一直游到海水變藍。」</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">𝔸𝕣𝕔𝕙𝕚𝕧𝕖𝕤</div><div class="length-num">112</div></a><a href="/tags/"><div class="headline">𝕋𝕒𝕘𝕤</div><div class="length-num">35</div></a><a href="/categories/"><div class="headline">𝔽𝕠𝕝𝕕𝕖𝕣𝕤</div><div class="length-num">17</div></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">用於備份小明的腦子。<br> ————<b> Tips </b>————<br> 在右下角可切換爲「简体中文」。<br> 部分評論從QQ空間或puq抓取，由於技術有限，無法顯示正確的位置和時間，望見諒。<br> ————<b> 本站常規欄目 </b>————<br> 周日中午:高中回憶《中外历史纲要》<br> 周二清晨:語錄體《主机註記》<br> 周三下午:有事大家谈/掷地有声<br> 周三/六晚上:算法學習筆記<br> ————<b> 計劃中 </b>————<br> 美食評測, 每日一圖, ...</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目錄</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-4"><a class="toc-link" href="#%C2%A74-3-3-%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5"><span class="toc-text">§4.3.3　协方差矩阵</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A6%82%E7%8E%87%E8%AE%BA%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%94%EF%BC%89"><span class="toc-text">概率论学习笔记（五）</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A6%82%E7%8E%87%E8%AE%BA%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AD%EF%BC%89"><span class="toc-text">概率论学习笔记（六）</span></a></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/0.html" title="無標題">無標題</a><time datetime="2024-06-14T07:10:44.736Z" title=" 2024-06-14 15:10:44">2024-06-14</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/0.html" title="無標題">無標題</a><time datetime="2024-06-14T04:53:50.589Z" title=" 2024-06-14 12:53:50">2024-06-14</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/0.html" title="無標題">無標題</a><time datetime="2024-06-14T04:49:55.663Z" title=" 2024-06-14 12:49:55">2024-06-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/d415b872.html" title="无废话极简笔记｜概率论与数理统计［简中］"><img src="/img/notes-gaoshu-cover.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="无废话极简笔记｜概率论与数理统计［简中］"/></a><div class="content"><a class="title" href="/posts/d415b872.html" title="无废话极简笔记｜概率论与数理统计［简中］">无废话极简笔记｜概率论与数理统计［简中］</a><time datetime="2024-06-14T04:38:58.000Z" title=" 2024-06-14 12:38:58">2024-06-14</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/e8dfbf5.html" title="2024WHCPC｜May.11 CUC2024区域赛重现#13">2024WHCPC｜May.11 CUC2024区域赛重现#13</a><time datetime="2024-06-02T08:05:22.740Z" title=" 2024-06-02 16:05:22">2024-06-02</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2024 By 小明同學</div><div class="footer_custom_text"><span id="runtime"></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="閱讀模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="簡繁轉換">繁</button><button id="darkmode" type="button" title="淺色和深色模式轉換"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="單欄和雙欄切換"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="設定"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目錄"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直達評論"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="返回頂部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="/js/tw_cn.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>(() => {
  const getCount = () => {
    const countELement = document.getElementById('twikoo-count')
    if(!countELement) return
    twikoo.getCommentsCount({
      envId: 'https://twikoo.kobicgend.top/',
      region: 'ap-shanghai',
      urls: [window.location.pathname],
      includeReply: false
    }).then(res => {
      countELement.textContent = res[0].count
    }).catch(err => {
      console.error(err)
    })
  }

  const init = () => {
    twikoo.init(Object.assign({
      el: '#twikoo-wrap',
      envId: 'https://twikoo.kobicgend.top/',
      region: 'ap-shanghai',
      onCommentLoaded: () => {
        btf.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.tk-owo-emotion)'))
      }
    }, null))

    
  }

  const loadTwikoo = () => {
    if (typeof twikoo === 'object') setTimeout(init,0)
    else getScript('https://cdn.jsdelivr.net/npm/twikoo@1.6.31/dist/twikoo.all.min.js').then(init)
  }

  if ('Twikoo' === 'Twikoo' || !false) {
    if (false) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo()
  } else {
    window.loadOtherComment = loadTwikoo
  }
})()</script></div><script src="https://npm.elemecdn.com/jquery@latest/dist/jquery.min.js"></script><script src="/js/flipcountdown.js"></script><script data-pjax src="/js/runtime.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --><script async src="//at.alicdn.com/t/font_2032782_8d5kxvn09md.js"></script><!-- hexo injector body_end end --></body></html>